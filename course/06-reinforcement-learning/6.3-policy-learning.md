# 6.3 策略学习

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

在强化学习中，除了基于价值的方法（如Q-Learning）通过学习价值函数来间接推导出策略外，还有一类方法是直接学习和优化策略本身，这类方法统称为**策略学习 (Policy Learning)** 或**策略梯度方法 (Policy Gradient Methods)** [1]。策略学习的目标是找到一个参数化的策略 $\pi_{\theta}(a|s)$，使得智能体在与环境交互时能够最大化期望累积奖励。

策略学习在机器人抓取与操作中具有重要意义，尤其是在处理连续动作空间和高维状态空间时。它能够直接输出动作的概率分布，从而更好地处理随机性策略，并且在某些情况下能够学习到比基于价值方法更复杂的行为 [2]。

## 2. 策略梯度 (Policy Gradient)

策略梯度方法的核心思想是通过梯度上升来优化策略参数 $\theta$，以最大化策略的性能目标 $J(\theta)$。性能目标通常定义为期望累积奖励：

$$ J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} [\sum_{t=0}^T \gamma^t R_t] $$

其中 $\tau$ 表示一条轨迹 $(s_0, a_0, r_0, s_1, a_1, r_1, ...)$，$\pi_{\theta}$ 是参数化的策略。

策略梯度的关键在于**策略梯度定理 (Policy Gradient Theorem)**，它提供了一种计算性能目标梯度的方法，而无需对环境动力学进行建模 [3]：

$$ \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} [\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) A^{\pi}(s_t, a_t)] $$

其中 $A^{\pi}(s_t, a_t)$ 是优势函数 (Advantage Function)，表示在状态 $s_t$ 采取动作 $a_t$ 比平均而言好多少。在实际应用中，优势函数通常用 $Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)$ 来估计，或者直接用累积奖励 $G_t$ 来代替（如REINFORCE算法）。

## 3. REINFORCE 算法

**REINFORCE** (Monte-Carlo Policy Gradient) 是最早也是最简单的策略梯度算法之一，由 Williams 于1992年提出 [4]。它是一种蒙特卡洛方法，通过完整的轨迹来估计梯度。

REINFORCE 算法的步骤如下：

1.  **采样轨迹**：智能体根据当前策略 $\pi_{\theta}$ 与环境交互，生成一条完整的轨迹 $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)$。
2.  **计算回报**：对于轨迹中的每个时间步 $t$，计算从该时间步开始的折扣累积奖励 $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$。
3.  **更新策略**：使用以下梯度更新策略参数 $\theta$：

    $$ \theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) G_t $$

    其中 $\alpha$ 是学习率。

**优点**：
-   **简单易实现**：概念直观，实现相对简单。
-   **无偏估计**：策略梯度估计是无偏的。

**缺点**：
-   **方差大**：由于使用蒙特卡洛方法，梯度估计的方差较大，导致训练不稳定。
-   **样本效率低**：需要完整的轨迹才能进行一次更新，且每次更新只使用一条轨迹的经验。

## 4. 深度确定性策略梯度 (Deep Deterministic Policy Gradient, DDPG)

**深度确定性策略梯度 (DDPG)** 是一种用于连续动作空间的无模型、异策略 Actor-Critic 算法，由 Lillicrap 等人于2015年提出 [5]。DDPG 结合了DQN的思想（经验回放、目标网络）和策略梯度方法，使其能够有效地处理高维连续控制任务。

DDPG 包含两个主要网络：

1.  **Actor 网络 (Actor Network)**：参数为 $\theta^{\mu}$，它是一个确定性策略 $\mu(s; \theta^{\mu})$，直接将状态映射到动作。Actor 的目标是学习一个能够最大化Q值的策略。
2.  **Critic 网络 (Critic Network)**：参数为 $\theta^Q$，它是一个Q函数 $Q(s, a; \theta^Q)$，用于评估 Actor 采取的动作。Critic 的目标是学习一个准确的Q值函数。

DDPG 还引入了两个关键技术来稳定训练：

-   **经验回放 (Experience Replay)**：与DQN类似，DDPG使用经验回放缓冲区来存储经验样本，打破数据相关性。
-   **目标网络 (Target Networks)**：DDPG为 Actor 和 Critic 各自维护一个目标网络（Target Actor Network $\mu'(s; \theta^{\mu'})$ 和 Target Critic Network $Q'(s, a; \theta^{Q'})$）。目标网络的参数通过软更新 (Soft Update) 的方式从主网络复制过来，即 $ \theta' \leftarrow \tau \theta + (1 - \tau) \theta' $，其中 $\tau$ 是一个很小的值。

**DDPG 的训练过程**：

-   **Critic 更新**：Critic 网络通过最小化以下损失函数来更新：

    $$ L(\theta^Q) = \mathbb{E}_{(s,a,r,s	) \sim B} [(r + \gamma Q'(s	, \mu'(s	; \theta^{\mu'}) ; \theta^{Q'}) - Q(s, a; \theta^Q))^2] $$

    其中 $B$ 是经验回放缓冲区。

-   **Actor 更新**：Actor 网络通过策略梯度来更新，目标是最大化 Critic 评估的Q值：

    $$ \nabla_{\theta^{\mu}} J \approx \mathbb{E}_{s \sim B} [\nabla_a Q(s, \mu(s; \theta^{\mu}); \theta^Q) \nabla_{\theta^{\mu}} \mu(s; \theta^{\mu})] $$

**优点**：
-   **处理连续动作空间**：DDPG能够直接学习连续动作空间的策略，非常适合机器人控制任务。
-   **异策略学习**：可以利用旧经验进行学习，提高了样本效率。
-   **结合了价值和策略学习的优势**：Actor学习策略，Critic评估策略，相互促进。

**缺点**：
-   **超参数敏感**：DDPG对超参数的选择比较敏感，训练可能不稳定。
-   **探索问题**：由于Actor是确定性策略，需要额外的噪声（如Ornstein-Uhlenbeck噪声）来保证足够的探索。

## 5. 机器人应用

策略学习方法，特别是DDPG及其后续改进算法（如TD3、SAC），在机器人抓取与操作领域取得了显著成功：

-   **连续控制任务**：例如，机械臂的轨迹跟踪、力控制、灵巧手操作等，这些任务的动作空间通常是连续的，策略梯度方法能够直接学习到连续的控制指令 [6]。
-   **复杂技能学习**：通过策略学习，机器人可以学习到复杂的抓取策略，例如在杂乱环境中抓取特定物体，或者执行需要精细协调的多步骤操作 [7]。
-   **Sim-to-Real 迁移**：策略梯度方法学习到的策略通常对环境变化具有一定的鲁棒性，有助于将仿真环境中训练的策略迁移到真实机器人上。

## 6. 代码示例 (REINFORCE)

以下是一个使用PyTorch实现的REINFORCE算法示例，用于解决OpenAI Gym中的CartPole-v1环境。

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# 定义策略网络
class PolicyNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return torch.softmax(self.fc2(x), dim=1)

def reinforce(env_name=\'CartPole-v1\', num_episodes=1000, learning_rate=1e-2, gamma=0.99):
    env = gym.make(env_name)
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n

    policy_net = PolicyNetwork(state_size, action_size)
    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)

    rewards_history = []

    print("--- REINFORCE 训练开始 ---")
    for episode in range(num_episodes):
        state, _ = env.reset()
        log_probs = []
        rewards = []

        while True:
            state_tensor = torch.from_numpy(state).float().unsqueeze(0)
            action_probs = policy_net(state_tensor)
            
            m = Categorical(action_probs) # 从概率分布中采样动作
            action = m.sample()
            
            log_probs.append(m.log_prob(action)) # 记录对数概率
            
            next_state, reward, done, _, _ = env.step(action.item())
            rewards.append(reward)
            
            state = next_state

            if done:
                break
        
        rewards_history.append(np.sum(rewards))

        # 计算折扣累积奖励 (Gt)
        G = []
        returns = 0
        for r in reversed(rewards):
            returns = r + gamma * returns
            G.insert(0, returns)
        G = torch.tensor(G)

        # 标准化奖励 (可选，用于减少方差)
        # G = (G - G.mean()) / (G.std() + 1e-9)

        # 计算损失并更新策略网络
        policy_loss = []
        for log_prob, g in zip(log_probs, G):
            policy_loss.append(-log_prob * g) # 策略梯度目标：最大化期望奖励
        
        optimizer.zero_grad()
        torch.stack(policy_loss).sum().backward()
        optimizer.step()

        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(rewards_history[-50:])
            print(f"Episode {episode+1}: Average Reward = {avg_reward:.2f}")
            if avg_reward >= env.spec.reward_threshold: # 达到环境的成功阈值
                print(f"环境已解决，平均奖励达到 {env.spec.reward_threshold}！")
                break
    print("--- REINFORCE 训练结束 ---")
    env.close()

if __name__ == "__main__":
    # 注意：CartPole-v1 的 reward_threshold 是 475.0
    # REINFORCE 可能需要较长时间才能达到这个阈值，甚至可能无法达到
    # 实际应用中通常会使用更高级的策略梯度算法或Actor-Critic方法
    reinforce(num_episodes=2000)
```

## 7. 参考资料

- [1] Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
- [2] Kober, J., Bagnell, J. A., & Peters, J. (2013). Reinforcement learning in robotics: A survey. *The International Journal of Robotics Research*, 32(11), 1238-1274.
- [3] Lilian Weng. (2018). *Policy Gradient Algorithms*. [URL](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)
- [4] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine Learning*, 8(3-4), 229-256.
- [5] Lillicrap, T. P., et al. (2016). Continuous control with deep reinforcement learning. *arXiv preprint arXiv:1509.02971*.
- [6] OpenAI. (n.d.). *Spinning Up in Deep RL*. [URL](https://spinningup.openai.com/en/latest/index.html)
- [7] Schulman, J., et al. (2015). High-dimensional continuous control using generalized advantage estimation. *arXiv preprint arXiv:1506.02438*.
