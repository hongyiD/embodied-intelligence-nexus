# 6.1 强化学习简介

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

**强化学习 (Reinforcement Learning, RL)** 是一种机器学习范式，它使智能体（Agent）能够在与环境（Environment）的交互中学习如何采取行动以最大化累积奖励 [1]。与监督学习（从带标签数据中学习）和无监督学习（从无标签数据中发现模式）不同，强化学习通过试错（Trial-and-Error）的方式进行学习，智能体根据其行为产生的奖励或惩罚来调整其策略。这种学习方式非常适合机器人抓取与操作等需要序列决策和与动态环境交互的任务 [2]。

在机器人领域，强化学习的目标是让机器人学习一个最优策略，使其能够自主地完成复杂任务，例如在未知环境中导航、抓取不同形状的物体或执行精细的操作。这种方法避免了对机器人行为进行显式编程的需要，使得机器人能够展现出更强的适应性和泛化能力。

## 2. 核心概念

强化学习的核心在于智能体、环境、状态、动作、奖励和策略等基本要素。

-   **智能体 (Agent)**：学习者和决策者，例如机器人。
-   **环境 (Environment)**：智能体所处的外部世界，它接收智能体的动作并返回新的状态和奖励。
-   **状态 (State, $s$)**：环境的当前配置，智能体基于状态做出决策。
-   **动作 (Action, $a$)**：智能体在给定状态下可以执行的操作。
-   **奖励 (Reward, $r$)**：环境对智能体动作的反馈信号，可以是正的（鼓励）或负的（惩罚）。智能体的目标是最大化长期累积奖励。
-   **策略 (Policy, $\pi$)**：智能体从状态到动作的映射，定义了智能体在给定状态下选择哪个动作。策略可以是确定性的 ($\\pi(s) = a$) 或随机性的 ($\\pi(a|s)$)。

强化学习的交互过程是一个马尔可夫决策过程 (Markov Decision Process, MDP)。

## 3. 马尔可夫决策过程 (MDP) 与部分可观测马尔可夫决策过程 (POMDP)

### 3.1 马尔可夫决策过程 (MDP)

**马尔可夫决策过程 (Markov Decision Process, MDP)** 是强化学习的数学基础，它提供了一个形式化的框架来建模序列决策问题 [3]。一个MDP由以下五元组定义：

-   $S$：有限状态集合。
-   $A$：有限动作集合。
-   $P(s'|s, a)$：状态转移概率，表示在状态 $s$ 采取动作 $a$ 后，转移到状态 $s'$ 的概率。
-   $R(s, a, s')$：奖励函数，表示从状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 所获得的即时奖励。
-   $\gamma$：折扣因子，取值范围 $[0, 1]$，用于衡量未来奖励的重要性。$\gamma$ 越接近0，智能体越关注即时奖励；越接近1，智能体越关注长期奖励。

MDP的核心是**马尔可夫性质 (Markov Property)**：当前状态 $s_t$ 包含了预测未来所需的所有信息，即未来只依赖于当前状态和当前动作，而与过去的状态和动作无关 [4]。

$$ P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1}|s_t, a_t) $$

### 3.2 部分可观测马尔可夫决策过程 (POMDP)

在许多现实世界的机器人任务中，智能体可能无法完全观测到环境的真实状态，只能获得部分观测 (Observation)。例如，机器人可能只有有限的传感器，或者传感器存在噪声。在这种情况下，MDP的假设不再成立，我们需要使用**部分可观测马尔可夫决策过程 (Partially Observable Markov Decision Process, POMDP)** 来建模 [5]。

POMDP在MDP的基础上增加了以下元素：

-   $\Omega$：有限观测集合。
-   $O(o|s', a)$：观测概率，表示在状态 $s'$ 采取动作 $a$ 后，观测到 $o$ 的概率。

在POMDP中，智能体不能直接知道当前状态 $s_t$，而是通过一系列历史观测 $o_0, o_1, ..., o_t$ 来维护一个关于当前状态的**信念 (Belief)** 分布 $b(s)$。智能体的策略不再是状态到动作的映射，而是信念到动作的映射 [6]。POMDP的求解通常比MDP更复杂。

## 4. 价值函数 (Value Function)

价值函数是强化学习中的核心概念，用于评估在给定状态下或遵循某个策略时，智能体能够获得的长期累积奖励。主要有两种价值函数：

### 4.1 状态价值函数 (State-Value Function, $V^{\pi}(s)$)

状态价值函数 $V^{\pi}(s)$ 表示在状态 $s$ 下，遵循策略 $\pi$ 所能获得的期望累积奖励（或称回报）。

$$ V^{\pi}(s) = \mathbb{E}_{\pi} [G_t | S_t = s] = \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s] $$

其中 $G_t$ 是从时间步 $t$ 开始的累积折扣奖励。

### 4.2 动作价值函数 (Action-Value Function, $Q^{\pi}(s, a)$)

动作价值函数 $Q^{\pi}(s, a)$ 表示在状态 $s$ 下，采取动作 $a$ 后，然后遵循策略 $\pi$ 所能获得的期望累积奖励。

$$ Q^{\pi}(s, a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi} [R_{t+1} + \gamma V^{\pi}(S_{t+1}) | S_t = s, A_t = a] $$

最优策略 $\pi^*$ 对应的最优状态价值函数 $V^*(s)$ 和最优动作价值函数 $Q^*(s, a)$ 满足贝尔曼最优方程 (Bellman Optimality Equation)：

$$ V^*(s) = \max_a Q^*(s, a) $$
$$ Q^*(s, a) = \mathbb{E} [R_{t+1} + \gamma V^*(S_{t+1}) | S_t = s, A_t = a] $$

## 5. 价值迭代 (Value Iteration) 与策略迭代 (Policy Iteration)

价值迭代和策略迭代是求解MDP中最优策略的两种经典动态规划算法 [7]。

### 5.1 策略迭代 (Policy Iteration)

策略迭代通过两个交替的步骤来寻找最优策略：

1.  **策略评估 (Policy Evaluation)**：给定一个策略 $\pi$，计算其对应的状态价值函数 $V^{\pi}(s)$。这通常通过迭代地应用贝尔曼期望方程来完成：

    $$ V_{k+1}^{\pi}(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V_k^{\pi}(s')] $$

2.  **策略改进 (Policy Improvement)**：根据当前策略的价值函数 $V^{\pi}(s)$，更新策略 $\pi$。新的策略选择在每个状态下能最大化动作价值的动作：

    $$ \pi'(s) = \arg\max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^{\pi}(s')] $$

这两个步骤交替进行，直到策略不再发生变化，此时得到的策略就是最优策略 [8]。

### 5.2 价值迭代 (Value Iteration)

价值迭代直接通过迭代地更新状态价值函数来寻找最优价值函数 $V^*(s)$，而无需显式地维护一个策略。它直接应用贝尔曼最优方程进行更新：

$$ V_{k+1}(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V_k(s')] $$

当 $V_k(s)$ 收敛到 $V^*(s)$ 时，最优策略可以通过对 $V^*(s)$ 进行一次贪婪策略改进来获得 [9]。

**价值迭代与策略迭代的比较**：

| 特性       | 价值迭代 (Value Iteration)                               | 策略迭代 (Policy Iteration)                               |
| :--------- | :------------------------------------------------------- | :------------------------------------------------------- |
| **核心思想** | 直接迭代更新价值函数，直到收敛到最优价值函数。           | 交替进行策略评估和策略改进，直到策略收敛。               |
| **计算复杂度** | 通常需要更多的迭代次数才能收敛，但每次迭代计算量较小。 | 每次策略评估可能需要多次迭代，但策略改进后收敛速度快。 |
| **显式策略** | 在价值函数收敛后，通过一次贪婪策略改进得到最优策略。   | 在每次策略改进步骤中显式更新策略。                     |
| **收敛性**   | 保证收敛到最优价值函数。                                 | 保证收敛到最优策略。                                     |

## 6. 强化学习算法概述

强化学习算法可以根据其学习方式和目标进行分类：

-   **基于模型 (Model-based) vs. 无模型 (Model-free)**：
    -   **基于模型**：智能体学习环境的动态模型（状态转移概率和奖励函数），然后利用模型进行规划。例如，动态规划方法（价值迭代、策略迭代）就是基于模型的。
    -   **无模型**：智能体不学习环境模型，而是直接从经验中学习策略或价值函数。例如，Q-Learning、SARSA、Policy Gradient等。

-   **同策略 (On-policy) vs. 异策略 (Off-policy)**：
    -   **同策略**：学习和改进的策略是同一个策略。例如，SARSA、Policy Gradient。
    -   **异策略**：学习的策略（目标策略）与用于生成经验的策略（行为策略）不同。例如，Q-Learning、DQN。

-   **基于价值 (Value-based) vs. 基于策略 (Policy-based) vs. Actor-Critic**：
    -   **基于价值**：学习价值函数，然后从价值函数中推导出策略。例如，Q-Learning、DQN。
    -   **基于策略**：直接学习策略，输出动作或动作的概率分布。例如，REINFORCE、A2C。
    -   **Actor-Critic**：结合了基于价值和基于策略的方法，Actor学习策略，Critic学习价值函数来评估Actor的动作。例如，A2C、A3C、DDPG。

## 7. 代码示例 (价值迭代)

以下是一个简单的Python代码示例，演示了价值迭代算法在一个小型网格世界 (Grid World) 中的应用。

```python
import numpy as np

class GridWorld:
    def __init__(self, size=4, gamma=0.9, living_reward=-0.1):
        self.size = size
        self.gamma = gamma
        self.living_reward = living_reward
        self.states = [(r, c) for r in range(size) for c in range(size)]
        self.num_states = len(self.states)
        self.actions = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}
        self.num_actions = len(self.actions)
        self.terminal_states = [(size-1, size-1)] # 目标状态
        self.rewards = {(size-1, size-1): 1.0} # 目标状态奖励

    def get_next_state_and_reward(self, state, action):
        r, c = state
        dr, dc = self.actions[action]
        next_r, next_c = r + dr, c + dc

        # 边界检查
        if not (0 <= next_r < self.size and 0 <= next_c < self.size):
            next_r, next_c = r, c # 撞墙则停留在原地
        
        next_state = (next_r, next_c)
        reward = self.rewards.get(next_state, self.living_reward)
        return next_state, reward

def value_iteration(env, theta=1e-6):
    V = {s: 0 for s in env.states} # 初始化价值函数
    
    while True:
        delta = 0
        for s in env.states:
            if s in env.terminal_states: # 终止状态价值为0
                V[s] = 0
                continue

            v = V[s]
            q_values = []
            for a in env.actions:
                next_state, reward = env.get_next_state_and_reward(s, a)
                q_values.append(reward + env.gamma * V[next_state])
            
            V[s] = max(q_values) # 更新状态价值
            delta = max(delta, abs(v - V[s]))
        
        if delta < theta: # 收敛条件
            break
            
    return V

def extract_policy(env, V):
    policy = {}
    for s in env.states:
        if s in env.terminal_states:
            policy[s] = 'Terminal'
            continue
        
        q_values = {}
        for a in env.actions:
            next_state, reward = env.get_next_state_and_reward(s, a)
            q_values[a] = reward + env.gamma * V[next_state]
        
        best_action = max(q_values, key=q_values.get)
        policy[s] = best_action
    return policy

if __name__ == "__main__":
    env = GridWorld()
    
    print("--- 价值迭代开始 ---")
    optimal_V = value_iteration(env)
    print("最优状态价值函数 V*:")
    for s in env.states:
        print(f"  状态 {s}: {optimal_V[s]:.2f}")

    optimal_policy = extract_policy(env, optimal_V)
    print("\n最优策略 π*:")
    for s in env.states:
        print(f"  状态 {s}: {optimal_policy[s]}")

    # 可视化策略 (概念性)
    print("\n--- 策略可视化 (概念性) ---")
    grid_policy = [['' for _ in range(env.size)] for _ in range(env.size)]
    for (r, c), action in optimal_policy.items():
        if action == 'up': grid_policy[r][c] = '↑'
        elif action == 'down': grid_policy[r][c] = '↓'
        elif action == 'left': grid_policy[r][c] = '←'
        elif action == 'right': grid_policy[r][c] = '→'
        elif action == 'Terminal': grid_policy[r][c] = 'T'
    
    for row in grid_policy:
        print(' '.join(row))
```

## 8. 参考资料

- [1] Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
- [2] Kober, J., Bagnell, J. A., & Peters, J. (2013). Reinforcement learning in robotics: A survey. *The International Journal of Robotics Research*, 32(11), 1238-1274.
- [3] Bellman, R. (1957). *Dynamic Programming*. Princeton University Press.
- [4] Puterman, M. L. (1994). *Markov Decision Processes: Discrete Stochastic Dynamic Programming*. John Wiley & Sons.
- [5] Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially observable stochastic domains. *Artificial Intelligence*, 101(1-2), 99-134.
- [6] Monahan, G. E. (1982). A survey of partially observable Markov decision processes: Theory, models, and algorithms. *Management Science*, 28(1), 1-16.
- [7] GeeksforGeeks. (2024). *Value Iteration vs. Policy Iteration*. [URL](https://www.geeksforgeeks.org/data-science/what-is-the-difference-between-value-iteration-and-policy-iteration/)
- [8] Howard, R. A. (1960). *Dynamic Programming and Markov Processes*. MIT Press.
- [9] Bellman, R. (1957). A Markovian decision process. *Journal of Mathematical Mechanics*, 6(5), 679-684.
