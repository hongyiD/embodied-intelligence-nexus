# 6.4 Actor-Critic 方法

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

**Actor-Critic** 方法是强化学习中一类重要的算法，它结合了**基于价值 (Value-based)** 和**基于策略 (Policy-based)** 方法的优点 [1]。在基于价值的方法（如Q-Learning）中，智能体学习一个价值函数，然后从该价值函数中推导出策略。在基于策略的方法（如REINFORCE）中，智能体直接学习一个参数化的策略。Actor-Critic 方法则同时学习一个策略（称为 **Actor**）和一个价值函数（称为 **Critic**），两者相互协作，共同优化学习过程 [2]。

这种混合方法在机器人抓取与操作等复杂任务中表现出色，尤其适用于连续动作空间和需要稳定学习的场景。Actor-Critic 能够利用 Critic 提供的价值估计来降低策略梯度估计的方差，从而使策略学习更加稳定和高效。

## 2. 核心原理

Actor-Critic 架构主要由两个核心组件构成：

1.  **Actor (策略网络)**：Actor 负责学习和输出策略 $\pi(a|s; \theta)$，其中 $\theta$ 是策略网络的参数。它决定了智能体在给定状态下应该采取什么动作。Actor 的目标是最大化期望累积奖励，并通过策略梯度方法进行更新。

2.  **Critic (价值网络)**：Critic 负责学习和输出价值函数，通常是状态价值函数 $V(s; \phi)$ 或动作价值函数 $Q(s, a; \phi)$，其中 $\phi$ 是价值网络的参数。Critic 的作用是评估 Actor 所采取动作的好坏，为 Actor 的策略更新提供指导信号。Critic 的更新通常通过最小化贝尔曼误差 (Bellman Error) 来实现。

**Actor 和 Critic 的交互**：

-   Actor 根据当前策略选择动作 $a_t$。
-   环境根据动作 $a_t$ 返回奖励 $r_t$ 和下一个状态 $s_{t+1}$。
-   Critic 使用 $r_t$ 和 $s_{t+1}$ 来更新其价值函数估计。
-   Critic 的价值估计（例如，通过计算**优势函数 (Advantage Function)**）被用来指导 Actor 的策略更新。Actor 会调整其策略，以增加那些被 Critic 评估为“好”的动作的概率，并减少“坏”动作的概率。

这种协同作用使得 Actor-Critic 方法能够结合策略梯度的探索能力和价值函数的稳定性，从而在学习效率和性能上取得平衡 [3]。

## 3. 关键方法/算法

### 3.1 优势函数 (Advantage Function)

优势函数 $A^{\pi}(s, a)$ 是 Actor-Critic 方法中的一个关键概念，它衡量了在给定状态 $s$ 下采取动作 $a$ 相对于该状态平均而言有多好。它的定义为：

$$ A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s) $$

优势函数在策略梯度更新中扮演着重要角色，因为它能够减少梯度估计的方差，从而使训练更加稳定。当 $A^{\pi}(s, a) > 0$ 时，表示动作 $a$ 比该状态下的平均动作更好，Actor 应该增加采取 $a$ 的概率；反之，则减少 [4]。

### 3.2 异步优势Actor-Critic (Asynchronous Advantage Actor-Critic, A3C)

**异步优势Actor-Critic (A3C)** 是由 Google DeepMind 于2016年提出的一种突破性算法，它通过并行化训练过程来显著提高学习效率和稳定性 [5]。A3C 的核心思想是：

-   **多线程并行**：A3C 启动多个独立的智能体（Worker），每个智能体都在自己的环境副本中并行地与环境交互。这些 Worker 独立地收集经验，并计算梯度。
-   **异步更新**：每个 Worker 在收集到一定量的经验后，会异步地将其梯度发送给一个共享的全局网络（Global Network），并更新全局网络的参数。然后，Worker 会从全局网络同步最新的参数，继续收集经验。
-   **优势函数**：每个 Worker 使用其本地的 Actor 和 Critic 来计算优势函数，并利用优势函数来更新策略和价值函数。

A3C 的异步更新机制使得训练过程更加稳定，因为它避免了数据之间的强相关性，并且多个 Worker 的经验多样性有助于更全面地探索环境。此外，A3C 不需要经验回放缓冲区，这简化了实现并减少了内存需求 [6]。

### 3.3 广义优势估计 (Generalized Advantage Estimation, GAE)

**广义优势估计 (Generalized Advantage Estimation, GAE)** 是由 Schulman 等人于2015年提出的一种技术，旨在为 Actor-Critic 算法提供一个更优的优势函数估计 [7]。GAE 通过引入一个超参数 $\lambda \in [0, 1]$，在优势估计的偏差和方差之间进行权衡：

$$ \hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V $$

其中 $\delta_t^V = r_t + \gamma V(s_{t+1}) - V(s_t)$ 是时间差分 (TD) 误差。

-   当 $\lambda = 0$ 时，GAE 退化为单步TD误差，具有低方差但高偏差。
-   当 $\lambda = 1$ 时，GAE 退化为蒙特卡洛优势估计，具有高方差但低偏差。

GAE 允许算法在两者之间找到一个最佳平衡点，从而提供更稳定和高效的策略梯度估计，这对于训练深度强化学习模型至关重要 [8]。

## 4. 机器人应用

Actor-Critic 方法及其变体在机器人抓取与操作领域得到了广泛应用，尤其是在需要处理连续动作空间和复杂动力学模型的任务中：

-   **连续控制**：例如，机械臂的精确轨迹跟踪、力矩控制、灵巧手操作等。Actor 可以直接输出连续的关节角度、速度或力矩指令 [9]。
-   **复杂技能学习**：通过 Actor-Critic，机器人可以学习到在非结构化环境中进行物体抓取、放置、推拉等复杂操作。Critic 能够有效地评估这些操作的长期价值。
-   **运动控制与步态学习**：在人形机器人或四足机器人的步态学习中，Actor-Critic 方法能够学习到平稳、高效的运动策略。
-   **Sim-to-Real 迁移**：由于 Actor-Critic 方法通常对环境噪声和模型不确定性具有一定的鲁棒性，因此在仿真环境中训练的策略可以更好地迁移到真实机器人上。

## 5. 代码示例 (概念性 Actor-Critic)

以下是一个概念性的Python代码示例，演示了 Actor-Critic 算法的基本结构。我们使用PyTorch实现一个简单的 Actor 和 Critic 网络，并在一个虚拟环境中进行训练。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np

# 模拟一个简单的环境
class SimpleEnv:
    def __init__(self):
        self.state = 0 # 初始状态
        self.max_steps = 10
        self.current_step = 0

    def step(self, action):
        reward = 0
        done = False
        
        if action == 0: # 向左
            self.state = max(0, self.state - 1)
        else: # 向右
            self.state = min(4, self.state + 1)
        
        if self.state == 4: # 达到目标状态
            reward = 1.0
            done = True
        elif self.current_step >= self.max_steps - 1: # 步数限制
            done = True

        self.current_step += 1
        return self.state, reward, done

    def reset(self):
        self.state = 0
        self.current_step = 0
        return self.state

# Actor 网络 (策略网络)
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )

    def forward(self, state):
        logits = self.net(state)
        return Categorical(logits=logits) # 输出动作的概率分布

# Critic 网络 (价值网络)
class Critic(nn.Module):
    def __init__(self, state_dim):
        super(Critic, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1) # 输出状态价值
        )

    def forward(self, state):
        return self.net(state)

def actor_critic_train(env, actor_net, critic_net, actor_optimizer, critic_optimizer, gamma=0.99, num_episodes=500):
    print("--- Actor-Critic 训练开始 ---")
    for episode in range(num_episodes):
        state = torch.tensor([env.reset()], dtype=torch.float32)
        log_probs = []
        rewards = []
        values = []
        done = False

        while not done:
            # Actor 选择动作
            dist = actor_net(state)
            action = dist.sample()
            log_prob = dist.log_prob(action)
            
            # Critic 评估状态价值
            value = critic_net(state)

            next_state_val, reward, done = env.step(action.item())
            next_state = torch.tensor([next_state_val], dtype=torch.float32)

            log_probs.append(log_prob)
            rewards.append(reward)
            values.append(value)
            
            state = next_state
        
        # 计算回报和优势
        returns = []
        R = 0
        for r in reversed(rewards):
            R = r + gamma * R
            returns.insert(0, R)
        returns = torch.tensor(returns, dtype=torch.float32)
        values = torch.cat(values).squeeze(-1)

        # 优势函数 (A = G - V)
        advantages = returns - values

        # 更新 Critic 网络
        critic_loss = (advantages**2).mean()
        critic_optimizer.zero_grad()
        critic_loss.backward()
        critic_optimizer.step()

        # 更新 Actor 网络
        actor_loss = (-torch.stack(log_probs) * advantages.detach()).mean() # detach() 避免梯度流回Critic
        actor_optimizer.zero_grad()
        actor_loss.backward()
        actor_optimizer.step()

        if (episode + 1) % 50 == 0:
            print(f"Episode {episode+1}: Total Reward = {sum(rewards):.2f}, Actor Loss = {actor_loss.item():.4f}, Critic Loss = {critic_loss.item():.4f}")
    print("--- Actor-Critic 训练结束 ---")

if __name__ == "__main__":
    env = SimpleEnv()
    state_dim = 1 # 状态是一个整数
    action_dim = 2 # 动作是向左或向右

    actor_net = Actor(state_dim, action_dim)
    critic_net = Critic(state_dim)

    actor_optimizer = optim.Adam(actor_net.parameters(), lr=1e-3)
    critic_optimizer = optim.Adam(critic_net.parameters(), lr=1e-2)

    actor_critic_train(env, actor_net, critic_net, actor_optimizer, critic_optimizer)

    # 部署和测试学习到的策略
    print("\n--- 测试学习到的策略 ---")
    test_state = torch.tensor([env.reset()], dtype=torch.float32)
    print(f"初始状态: {test_state.item()}")
    for _ in range(env.max_steps):
        dist = actor_net(test_state)
        action = dist.sample()
        print(f"  选择动作: {"向左" if action.item() == 0 else "向右"}")
        next_state_val, reward, done = env.step(action.item())
        test_state = torch.tensor([next_state_val], dtype=torch.float32)
        print(f"  新状态: {test_state.item()}, 奖励: {reward}")
        if done:
            print("  任务完成！")
            break
```

## 6. 参考资料

- [1] Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
- [2] Kober, J., Bagnell, J. A., & Peters, J. (2013). Reinforcement learning in robotics: A survey. *The International Journal of Robotics Research*, 32(11), 1238-1274.
- [3] Weng, L. (2018). *Policy Gradient Algorithms*. [URL](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)
- [4] Schulman, J., et al. (2015). High-dimensional continuous control using generalized advantage estimation. *arXiv preprint arXiv:1506.02438*.
- [5] Mnih, V., et al. (2016). Asynchronous methods for deep reinforcement learning. *International Conference on Machine Learning (ICML)*.
- [6] GeeksforGeeks. (2025). *Asynchronous Advantage Actor Critic (A3C) algorithm*. [URL](https://www.geeksforgeeks.org/machine-learning/asynchronous-advantage-actor-critic-a3c-algorithm/)
- [7] Schulman, J., et al. (2015). High-dimensional continuous control using generalized advantage estimation. *arXiv preprint arXiv:1506.02438*.
- [8] Medium. (2023). *Generalized Advantage Estimation (GAE): A Deep Dive into Bias-Variance and Policy Gradients*. [URL](https://shivang-ahd.medium.com/generalized-advantage-estimation-a-deep-dive-into-bias-variance-and-policy-gradients-a5e0b3454dad)
- [9] Pane, Y. P., et al. (2016). Actor-critic reinforcement learning for tracking control in robotic manipulators. *IEEE International Conference on Robotics and Automation (ICRA)*.
