# 6.2 Q-Learning

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

**Q-Learning** 是强化学习领域中最重要且最基础的算法之一，由 Watkins 于1989年提出 [1]。它是一种**无模型 (Model-Free)**、**异策略 (Off-Policy)** 的**价值迭代 (Value Iteration)** 算法，旨在学习一个最优的动作价值函数 $Q^*(s, a)$，从而指导智能体在给定状态 $s$ 时选择最优动作 $a$。Q-Learning 的核心思想是，智能体通过与环境的交互，不断更新其对每个状态-动作对的“质量”评估（即Q值），最终收敛到最优策略 [2]。

在机器人抓取与操作等任务中，Q-Learning 能够让机器人通过试错学习，自主发现如何在复杂环境中执行任务，而无需预先建立精确的环境模型。这对于处理环境不确定性高、动力学模型难以精确建模的机器人系统尤为重要。

## 2. 核心原理

Q-Learning 的核心是**Q函数 (Q-function)**，记作 $Q(s, a)$。$Q(s, a)$ 表示在状态 $s$ 下采取动作 $a$，然后遵循最优策略所能获得的期望最大累积奖励。智能体的目标是学习到最优的Q函数 $Q^*(s, a)$，一旦 $Q^*(s, a)$ 已知，最优策略 $\pi^*(s)$ 就可以通过在每个状态 $s$ 下选择使 $Q^*(s, a)$ 最大的动作来获得：

$$ \pi^*(s) = \arg\max_a Q^*(s, a) $$

Q-Learning 算法通过迭代更新Q值来逼近 $Q^*(s, a)$。其更新规则基于贝尔曼最优方程 (Bellman Optimality Equation) [3]：

$$ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] $$

其中：
-   $Q(s, a)$：当前状态 $s$ 下采取动作 $a$ 的Q值。
-   $\alpha$：学习率 (Learning Rate)，控制每次更新的步长，取值范围 $[0, 1]$。
-   $r$：在状态 $s$ 采取动作 $a$ 后获得的即时奖励。
-   $\gamma$：折扣因子 (Discount Factor)，控制未来奖励的重要性，取值范围 $[0, 1]$。
-   $s'$：在状态 $s$ 采取动作 $a$ 后转移到的下一个状态。
-   $\max_{a'} Q(s', a')$：在下一个状态 $s'$ 下，所有可能动作 $a'$ 中最大的Q值，代表了从 $s'$ 开始能获得的最佳未来奖励。

这个更新规则的含义是，智能体根据当前Q值、即时奖励以及对未来最佳Q值的估计来调整当前Q值。通过不断地与环境交互和更新，Q值会逐渐收敛到最优Q值 $Q^*(s, a)$。

## 3. 价值迭代与Q-Learning

Q-Learning 可以被视为一种**异步的价值迭代**方法 [4]。在价值迭代中，我们迭代地更新状态价值函数 $V(s)$。而在Q-Learning中，我们迭代地更新动作价值函数 $Q(s, a)$。Q-Learning 的更新公式直接反映了贝尔曼最优方程的思想，即当前状态-动作对的Q值应该等于即时奖励加上折扣后的下一个状态的最大Q值。

Q-Learning 的异策略特性意味着它可以使用一个**行为策略 (Behavior Policy)**（例如 $\epsilon$-贪婪策略，用于探索环境）来生成经验，同时学习一个**目标策略 (Target Policy)**（贪婪策略，用于最大化Q值）。这使得Q-Learning 能够在探索的同时学习最优策略，而无需行为策略本身是最优的 [5]。

## 4. Fitted Q Iteration

传统的Q-Learning算法通常适用于状态和动作空间都是离散且有限的情况。然而，在许多实际的机器人任务中，状态空间（如关节角度、末端执行器位置）和动作空间（如力矩、速度指令）往往是连续的，或者离散但维度非常高。在这种情况下，使用表格来存储Q值变得不可行。

**Fitted Q Iteration (FQI)** 是一种将Q-Learning与函数逼近器（如神经网络、决策树等）结合的方法，用于处理连续或高维状态/动作空间 [6]。FQI 的基本思想是：

1.  **数据收集**：智能体与环境交互，收集一系列经验样本 $(s_t, a_t, r_t, s_{t+1})$。
2.  **目标Q值计算**：对于每个经验样本，计算其目标Q值 $y_t = r_t + \gamma \max_{a'} Q(s_{t+1}, a')$。这里的 $Q(s_{t+1}, a')$ 是由当前的Q函数逼近器给出的。
3.  **训练函数逼近器**：将 $(s_t, a_t)$ 作为输入，将 $y_t$ 作为目标输出，训练函数逼近器来拟合 $Q(s, a) \approx y$。这通常是一个监督学习问题。
4.  **迭代**：重复上述步骤，不断收集新数据并更新Q函数逼近器，直到收敛。

FQI 使得Q-Learning能够扩展到更复杂的连续控制问题，但其性能高度依赖于所选函数逼近器的能力和训练数据的质量。

## 5. 深度Q网络 (Deep Q-Network, DQN)

**深度Q网络 (Deep Q-Network, DQN)** 是 DeepMind 于2013年提出的一种突破性算法，它成功地将Q-Learning与深度神经网络结合，解决了传统Q-Learning在处理高维感知输入（如图像）时的挑战 [7]。DQN 的成功使得强化学习在玩Atari游戏等任务上取得了超人类的表现。

DQN 引入了两个关键技术来稳定深度Q网络的训练：

1.  **经验回放 (Experience Replay)**：智能体将与环境交互产生的经验 $(s_t, a_t, r_t, s_{t+1})$ 存储在一个**回放缓冲区 (Replay Buffer)** 中。在训练时，DQN 从这个缓冲区中随机采样一批经验来更新网络。这样做有几个好处：
    -   **打破数据相关性**：连续的经验样本之间存在高度相关性，直接用于训练会导致网络不稳定。随机采样打破了这种相关性。
    -   **提高数据利用率**：每个经验样本可以被多次用于训练，提高了数据效率。
    -   **避免灾难性遗忘**：通过重用旧经验，可以防止网络在学习新经验时遗忘旧知识。

2.  **目标网络 (Target Network)**：DQN 使用两个结构相同的神经网络：**当前Q网络 (Current Q-Network)** $Q(s, a; \theta)$ 和**目标Q网络 (Target Q-Network)** $Q(s, a; \theta^-)$。当前Q网络用于计算当前Q值，并进行实时更新。目标Q网络用于计算目标Q值 $r + \gamma \max_{a'} Q(s', a'; \theta^-)$。目标Q网络的参数 $\theta^-$ 会周期性地从当前Q网络复制过来，但在两次复制之间保持固定。这样做可以稳定训练目标，避免因目标Q值频繁变化而导致的训练震荡 [8]。

DQN 的损失函数通常是均方误差：

$$ L(\theta) = \mathbb{E}_{(s,a,r,s') \sim U(D)} [(r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))^2] $$

其中 $U(D)$ 表示从经验回放缓冲区 $D$ 中均匀采样。

## 6. Q-Learning 在机器人中的应用

Q-Learning 及其变体（尤其是DQN）在机器人抓取与操作领域展现了巨大的潜力。它允许机器人从原始传感器数据（如图像）直接学习复杂的控制策略，而无需人工特征工程 [9]。

**应用案例**：

-   **物体抓取**：机器人可以通过DQN学习从图像中识别可抓取区域，并生成相应的抓取动作。例如，DeepMind 的研究展示了DQN如何让机器人学习抓取各种日常物品 [10]。
-   **导航与避障**：机器人可以学习在复杂环境中导航，避开障碍物，并到达目标位置。
-   **灵巧操作**：虽然挑战更大，但DQN也被用于学习精细的灵巧操作任务，例如积木堆叠或工具使用。

**挑战**：

-   **样本效率**：DQN通常需要大量的交互数据才能训练出高性能的策略，这在真实机器人上收集成本高昂且耗时。
-   **泛化能力**：学习到的策略可能难以泛化到未见过的新物体或新环境。
-   **安全性**：在试错学习过程中，机器人可能会采取危险动作，损坏自身或环境。
-   **连续动作空间**：DQN最初设计用于离散动作空间。对于连续动作空间，需要进行修改，例如使用离散化、或者结合策略梯度方法（如DDPG）。

## 7. 代码示例 (Q-Learning)

以下是一个简单的Python代码示例，演示了Q-Learning算法在一个小型网格世界 (Grid World) 中的应用。这个例子展示了Q表的初始化、动作选择（$\epsilon$-贪婪策略）和Q值更新过程。

```python
import numpy as np
import random

class GridWorld:
    def __init__(self, size=4, gamma=0.9, alpha=0.1, epsilon=0.1, living_reward=-0.1):
        self.size = size
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.living_reward = living_reward
        
        self.states = [(r, c) for r in range(size) for c in range(size)]
        self.num_states = len(self.states)
        self.state_to_idx = {s: i for i, s in enumerate(self.states)}
        self.idx_to_state = {i: s for i, s in enumerate(self.states)}

        self.actions = {
            0: (0, 1),  # Right
            1: (0, -1), # Left
            2: (1, 0),  # Down
            3: (-1, 0)  # Up
        }
        self.action_names = {0: 'Right', 1: 'Left', 2: 'Down', 3: 'Up'}
        self.num_actions = len(self.actions)

        self.terminal_state = (size-1, size-1) # 目标状态
        self.rewards = {self.terminal_state: 1.0} # 目标状态奖励

        # 初始化Q表
        self.Q = np.zeros((self.num_states, self.num_actions))

    def get_next_state_and_reward(self, current_state_idx, action_idx):
        r, c = self.idx_to_state[current_state_idx]
        dr, dc = self.actions[action_idx]
        next_r, next_c = r + dr, c + dc

        # 边界检查
        if not (0 <= next_r < self.size and 0 <= next_c < self.size):
            next_r, next_c = r, c # 撞墙则停留在原地
        
        next_state = (next_r, next_c)
        next_state_idx = self.state_to_idx[next_state]
        
        reward = self.rewards.get(next_state, self.living_reward)
        return next_state_idx, reward

    def choose_action(self, state_idx):
        # epsilon-greedy 策略
        if random.uniform(0, 1) < self.epsilon:
            return random.randint(0, self.num_actions - 1) # 探索
        else:
            return np.argmax(self.Q[state_idx, :]) # 利用

    def learn(self, num_episodes=1000):
        print("--- Q-Learning 训练开始 ---")
        for episode in range(num_episodes):
            current_state_idx = self.state_to_idx[(0, 0)] # 每次从起点开始
            done = False
            total_reward = 0

            while not done:
                action_idx = self.choose_action(current_state_idx)
                next_state_idx, reward = self.get_next_state_and_reward(current_state_idx, action_idx)

                # Q值更新
                old_q_value = self.Q[current_state_idx, action_idx]
                next_max_q = np.max(self.Q[next_state_idx, :])
                new_q_value = old_q_value + self.alpha * (reward + self.gamma * next_max_q - old_q_value)
                self.Q[current_state_idx, action_idx] = new_q_value

                current_state_idx = next_state_idx
                total_reward += reward

                if self.idx_to_state[current_state_idx] == self.terminal_state:
                    done = True
            
            if (episode + 1) % 100 == 0:
                print(f"Episode {episode+1}: Total Reward = {total_reward:.2f}")
        print("--- Q-Learning 训练结束 ---")

    def get_optimal_policy(self):
        policy = {}
        for state_idx in range(self.num_states):
            state = self.idx_to_state[state_idx]
            if state == self.terminal_state:
                policy[state] = 'Terminal'
            else:
                best_action_idx = np.argmax(self.Q[state_idx, :])
                policy[state] = self.action_names[best_action_idx]
        return policy

if __name__ == "__main__":
    env = GridWorld()
    env.learn(num_episodes=2000)

    optimal_policy = env.get_optimal_policy()
    print("\n最优策略 π*:")
    grid_policy_display = [['' for _ in range(env.size)] for _ in range(env.size)]
    for (r, c), action in optimal_policy.items():
        if action == 'Right': grid_policy_display[r][c] = '→'
        elif action == 'Left': grid_policy_display[r][c] = '←'
        elif action == 'Down': grid_policy_display[r][c] = '↓'
        elif action == 'Up': grid_policy_display[r][c] = '↑'
        elif action == 'Terminal': grid_policy_display[r][c] = 'T'
    
    for row in grid_policy_display:
        print(' '.join(row))

    print("\nQ表 (部分展示):")
    for i in range(min(5, env.num_states)):
        state = env.idx_to_state[i]
        print(f"状态 {state}: {env.Q[i, :].round(2)}")
```

## 8. 参考资料

- [1] Watkins, C. J. C. H. (1989). *Learning from Delayed Rewards*. PhD thesis, King's College, Cambridge.
- [2] Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
- [3] Bellman, R. (1957). *Dynamic Programming*. Princeton University Press.
- [4] GeeksforGeeks. (2025). *Q-Learning in Reinforcement Learning*. [URL](https://www.geeksforgeeks.org/machine-learning/q-learning-in-python/)
- [5] Gibberblot. (n.d.). *Value-based methods*. Mastering Reinforcement Learning. [URL](https://gibberblot.github.io/rl-notes/single-agent/value-based.html)
- [6] Mnih, V., et al. (2013). Playing Atari with Deep Reinforcement Learning. *arXiv preprint arXiv:1312.5602*.
- [7] Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533.
- [8] Hasselt, H. V., Guez, A., & Silver, D. (2016). Deep Reinforcement Learning with Double Q-learning. *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)*.
- [9] Kober, J., Bagnell, J. A., & Peters, J. (2013). Reinforcement learning in robotics: A survey. *The International Journal of Robotics Research*, 32(11), 1238-1274.
- [10] Levine, S., et al. (2016). End-to-end training of deep visuomotor policies. *Journal of Machine Learning Research*, 17(39), 1-40.
