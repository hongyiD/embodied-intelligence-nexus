# 5.5 逆强化学习

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

**逆强化学习 (Inverse Reinforcement Learning, IRL)**，也称为**逆最优控制 (Inverse Optimal Control)**，是模仿学习领域的一个高级范式。与行为克隆 (Behavior Cloning, BC) 直接学习状态到动作的映射不同，IRL的目标是**从专家的演示中推断出专家所遵循的奖励函数 (Reward Function)** [1]。其核心假设是专家是根据某个（未知）最优奖励函数来行动的，即专家的行为是最大化其累积奖励的结果。

一旦学习到了这个奖励函数，就可以利用标准的强化学习 (Reinforcement Learning, RL) 算法来学习一个最优策略。IRL的优势在于，奖励函数通常比策略本身更具有泛化性，能够更好地适应环境变化，甚至可能学习到超越专家演示的策略 [2]。

## 2. 核心原理

在传统的强化学习中，奖励函数是已知的，目标是找到一个策略来最大化累积奖励。而IRL则反其道而行之：给定专家策略（通过演示数据体现），目标是找到一个奖励函数，使得该专家策略在该奖励函数下是（近似）最优的 [3]。

IRL的基本思想可以概括为：

1.  **专家最优性假设**：IRL的核心假设是专家行为是根据某个潜在的奖励函数进行最优决策的。这意味着专家在每个状态下选择的动作，都是为了最大化未来的累积奖励。
2.  **奖励函数推断**：IRL算法通过分析专家演示中的状态-动作序列，尝试找出最能解释这些行为的奖励函数。例如，如果专家总是避开某个区域，那么这个区域可能对应着负奖励；如果专家总是快速达到某个目标，那么这个目标可能对应着正奖励。
3.  **策略生成**：一旦推断出奖励函数，就可以将其作为标准强化学习问题的输入，使用Q-Learning、Policy Gradient等RL算法来学习一个在该奖励函数下最优的策略。这个策略理论上可以复制甚至超越专家的表现。

$$ R^* = \arg\max_R \mathbb{E}_{\pi_E} [\sum_{t=0}^T \gamma^t R(s_t, a_t)] - \mathbb{E}_{\pi \neq \pi_E} [\sum_{t=0}^T \gamma^t R(s_t, a_t)] $$

上述公式（简化形式）表示IRL的目标是找到一个奖励函数 $R^*$，使得专家策略 $\pi_E$ 在该奖励函数下的累积奖励高于任何其他非专家策略 $\pi$ [4]。

## 3. 关键方法/算法

IRL领域发展出了多种算法，其中一些经典方法包括：

### 3.1 最大熵逆强化学习 (Maximum Entropy IRL, MaxEnt IRL)

MaxEnt IRL是IRL中最流行的方法之一。它假设专家行为不仅是最大化累积奖励，而且在所有能产生相同特征期望的策略中，专家策略是熵最大的（即最随机的）[5]。这种“最大熵”原则有助于解决奖励函数不唯一的问题，因为它偏好那些能够解释专家行为，同时又不过度拟合专家数据的奖励函数。

MaxEnt IRL的目标是找到一个奖励函数，使得专家轨迹的特征期望与所有可能的策略的特征期望相匹配，同时最大化策略的熵。

### 3.2 贝叶斯逆强化学习 (Bayesian IRL)

贝叶斯IRL将奖励函数视为一个随机变量，并使用贝叶斯推断来计算奖励函数的后验分布 [6]。这种方法能够处理专家演示中的不确定性，并提供对奖励函数估计的置信度。它允许我们考虑多个可能的奖励函数，而不是仅仅找到一个点估计。

### 3.3 学徒学习 (Apprenticeship Learning)

学徒学习是一种结合了IRL和RL的方法。它迭代地进行以下步骤：

1.  **IRL步骤**：从专家演示中推断出一个奖励函数。
2.  **RL步骤**：使用推断出的奖励函数训练一个策略。
3.  **评估**：评估新策略的表现，如果它比专家策略差，则调整奖励函数或继续迭代。

这种方法旨在找到一个能够匹配专家表现甚至超越专家的策略。

## 4. 优点

-   **泛化能力强**：学习到的奖励函数通常比直接学习的策略更具泛化性，能够更好地适应环境变化或任务变体 [7]。
-   **可解释性**：通过奖励函数，可以更好地理解专家行为背后的动机和目标，这对于调试和改进机器人行为非常有帮助。
-   **可能超越专家**：一旦学习到准确的奖励函数，标准的强化学习算法理论上可以找到比专家演示更优的策略，因为RL可以进行更广泛的探索。
-   **减少对奖励函数设计的依赖**：避免了手工设计复杂奖励函数的困难，尤其是在任务目标难以量化时。

## 5. 缺点

-   **计算复杂性高**：IRL通常需要嵌套强化学习算法，导致计算成本远高于行为克隆 [8]。
-   **奖励函数模糊性**：可能存在多个奖励函数都能解释专家行为，导致学习到的奖励函数不唯一。MaxEnt IRL等方法试图缓解这个问题，但仍是一个挑战。
-   **专家最优性假设**：IRL依赖于专家行为是（近似）最优的假设。如果专家行为次优或包含错误，IRL可能会推断出错误的奖励函数。
-   **数据需求**：虽然比纯RL更高效，但IRL仍然需要高质量的专家演示数据。

## 6. 代码示例 (概念性IRL)

以下是一个概念性的Python代码片段，用于说明逆强化学习的基本思想。我们模拟一个简单的环境和专家行为，然后尝试推断出专家可能遵循的奖励函数。这里我们使用一个非常简化的线性奖励函数模型。

```python
import numpy as np

# 模拟环境和状态特征
class Environment:
    def __init__(self):
        self.states = np.array([
            [0, 0], [0, 1], [0, 2], 
            [1, 0], [1, 1], [1, 2], 
            [2, 0], [2, 1], [2, 2]
        ]) # 9个离散状态 (x, y)
        self.num_states = len(self.states)
        self.actions = {0: "up", 1: "down", 2: "left", 3: "right"}
        self.num_actions = len(self.actions)

    def get_next_state(self, current_state_idx, action_idx):
        current_pos = self.states[current_state_idx]
        next_pos = np.copy(current_pos)
        if action_idx == 0: next_pos[1] += 1 # up
        elif action_idx == 1: next_pos[1] -= 1 # down
        elif action_idx == 2: next_pos[0] -= 1 # left
        elif action_idx == 3: next_pos[0] += 1 # right
        
        # 边界检查
        next_pos = np.clip(next_pos, 0, 2)
        
        # 找到下一个状态的索引
        for i, state in enumerate(self.states):
            if np.array_equal(state, next_pos):
                return i
        return current_state_idx # 如果没找到，保持原状态 (不应该发生)

# 模拟专家策略 (假设专家总是想去状态 (2,2) 目标)
class ExpertPolicy:
    def __init__(self, env):
        self.env = env
        self.target_state_idx = 8 # (2,2) 的索引

    def get_action(self, current_state_idx):
        current_pos = self.env.states[current_state_idx]
        target_pos = self.env.states[self.target_state_idx]
        
        # 简单的贪婪策略：朝目标移动
        best_action = -1
        min_dist = float('inf')
        for action_idx in range(self.env.num_actions):
            next_state_idx = self.env.get_next_state(current_state_idx, action_idx)
            next_pos = self.env.states[next_state_idx]
            dist = np.linalg.norm(next_pos - target_pos)
            if dist < min_dist:
                min_dist = dist
                best_action = action_idx
        return best_action

# 概念性IRL算法：推断线性奖励函数
def conceptual_irl(expert_trajectories, env, feature_extractor, gamma=0.9):
    # 假设奖励函数是状态特征的线性组合: R(s) = w * phi(s)
    # 目标是找到权重 w

    # 收集专家轨迹的特征期望
    expert_feature_expectations = np.zeros(feature_extractor(env.states[0]).shape)
    for trajectory in expert_trajectories:
        for state_idx, action_idx in trajectory:
            expert_feature_expectations += feature_extractor(env.states[state_idx])
    expert_feature_expectations /= len(expert_trajectories) # 简化平均

    # 在实际IRL中，这里会有一个迭代过程，
    # 1. 假设一个奖励函数 R(s) = w * phi(s)
    # 2. 使用RL求解在该R下的最优策略 pi_w
    # 3. 计算 pi_w 的特征期望
    # 4. 调整 w，使得 pi_w 的特征期望接近专家特征期望
    # 这是一个复杂的优化问题，这里仅作概念性演示

    print("\n--- 概念性IRL：推断奖励函数 ---")
    print("专家特征期望 (简化):", expert_feature_expectations.round(2))
    print("假设我们通过优化，找到了一个奖励函数权重 w，使得专家行为最优。")
    # 假设推断出的权重 w
    inferred_weights = np.array([1.0, 1.0]) # 假设 (2,2) 目标意味着 x,y 越大奖励越高
    print("推断出的奖励函数权重 w (简化):", inferred_weights.round(2))

    # 验证推断出的奖励函数
    print("\n验证推断出的奖励函数：")
    for i in range(env.num_states):
        state_features = feature_extractor(env.states[i])
        reward = np.dot(inferred_weights, state_features)
        print(f"状态 {env.states[i]}: 奖励 = {reward:.2f}")

    return inferred_weights

# 状态特征提取器 (例如，状态坐标本身作为特征)
def state_feature_extractor(state):
    return state

if __name__ == "__main__":
    env = Environment()
    expert_policy = ExpertPolicy(env)

    # 1. 收集专家演示轨迹
    print("--- 收集专家演示轨迹 ---")
    expert_trajectories = []
    for _ in range(5): # 模拟5条专家轨迹
        trajectory = []
        current_state_idx = np.random.randint(0, env.num_states) # 随机起始状态
        for step in range(10): # 每条轨迹10步
            action_idx = expert_policy.get_action(current_state_idx)
            trajectory.append((current_state_idx, action_idx))
            next_state_idx = env.get_next_state(current_state_idx, action_idx)
            current_state_idx = next_state_idx
            if current_state_idx == expert_policy.target_state_idx: break
        expert_trajectories.append(trajectory)
    
    # 打印部分专家轨迹
    for i, traj in enumerate(expert_trajectories[:2]):
        print(f"轨迹 {i+1}: {[env.states[s_idx] for s_idx, _ in traj]}")

    # 2. 概念性IRL推断奖励函数
    inferred_reward_weights = conceptual_irl(expert_trajectories, env, state_feature_extractor)

    # 3. (可选) 使用推断出的奖励函数进行强化学习
    print("\n--- 使用推断出的奖励函数进行强化学习 (概念性) ---")
    print("在实际中，这里会运行一个完整的RL算法（如Q-Learning或Policy Gradient），")
    print("使用 inferred_reward_weights 来计算每个状态的奖励，从而学习最优策略。")
    # 例如，一个状态的奖励可以计算为 np.dot(inferred_reward_weights, state_feature_extractor(state))
```

## 7. 参考资料

- [1] Ng, A. Y., & Russell, S. J. (2000). Algorithms for inverse reinforcement learning. *Proceedings of the Seventeenth International Conference on Machine Learning (ICML)*. [PDF](https://ai.stanford.edu/~ang/papers/icml00-irl.pdf)
- [2] Ab Azar, N., & Ahmad, M. N. (2020). From inverse optimal control to inverse reinforcement learning: A review. *Artificial Intelligence Review*, 53(7), 5037-5062.
- [3] Russell, S. J. (1998). Learning agents for uncertain environments (extended abstract). *Proceedings of the Eleventh Annual Conference on Computational Learning Theory (COLT)*.
- [4] Ng, A. Y., & Russell, S. J. (2000). Algorithms for inverse reinforcement learning. *Machine Learning*, 89(1-2), 1-49.
- [5] Ziebart, B. D., et al. (2008). Maximum entropy inverse reinforcement learning. *Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI)*.
- [6] Ramachandran, D., & Amir, E. (2007). Bayesian inverse reinforcement learning. *Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI)*.
- [7] The Gradient. (2018). *Learning from humans: what is inverse reinforcement learning?*. [URL](https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/)
- [8] LessWrong. (2022). *A Survey of Foundational Methods in Inverse Reinforcement Learning*. [URL](https://www.lesswrong.com/posts/wf83tBACPM9aiykPn/a-survey-of-foundational-methods-in-inverse-reinforcement)
