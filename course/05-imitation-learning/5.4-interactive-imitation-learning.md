# 5.4 交互式模仿学习

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

在行为克隆 (Behavior Cloning, BC) 中，一个主要挑战是**协变量偏移 (Covariate Shift)**，即机器人执行过程中可能进入训练数据中未曾出现过的状态，导致误差累积并偏离专家轨迹。为了解决这一问题，**交互式模仿学习 (Interactive Imitation Learning, IIL)** 应运而生 [1]。

交互式模仿学习的核心思想是在机器人学习和执行任务的过程中，引入人类专家的实时或间歇性干预。当机器人遇到不确定或偏离轨迹的状态时，专家可以提供即时反馈或演示，从而帮助机器人纠正错误，并学习在更广泛的状态空间中做出正确决策 [2]。这种方法有效地将在线学习的优势与模仿学习的简单性结合起来。

## 2. 核心原理

交互式模仿学习通常采用迭代的方式进行。在一个典型的交互式模仿学习循环中，机器人首先根据当前学习到的策略执行任务。当机器人表现不佳或进入未知状态时，专家会介入，提供正确的动作或演示。这些新的状态-动作对被添加到训练数据集中，然后策略会进行更新，从而提高机器人在这些“困难”状态下的表现 [3]。

这种交互机制使得学习到的策略能够更好地适应机器人自身的执行误差和环境变化，有效缓解了协变量偏移问题。

## 3. 关键方法/算法

### 3.1 DAgger (Dataset Aggregation)

**DAgger (Dataset Aggregation)** 是最著名和最有效的交互式模仿学习算法之一，由 Ross 等人于2011年提出 [4]。DAgger通过迭代地收集数据和重新训练策略来解决行为克隆中的协变量偏移问题。

DAgger算法的流程如下：

1.  **初始化**：
    -   收集一个初始的专家演示数据集 $D_0 = \{(s_i, a_i)\}$。
    -   使用 $D_0$ 训练一个初始策略 $\pi_0$（例如，通过行为克隆）。

2.  **迭代过程**：对于 $k = 0, 1, 2, ..., N$：
    a.  **策略执行**：让当前策略 $\pi_k$ 在环境中执行任务。记录机器人访问到的状态序列 $s_0, s_1, ..., s_T$。
    b.  **专家标注**：对于机器人访问到的每个状态 $s_t$，请求专家提供正确的动作 $a_t^{expert}$。这通常意味着专家需要实时纠正机器人的行为，或者在离线时对机器人轨迹中的状态进行标注。
    c.  **数据聚合**：将新收集到的状态-专家动作对 $D_k = \{(s_t, a_t^{expert})\}$ 添加到聚合数据集 $D_{agg} = D_{agg} \cup D_k$ 中。
    d.  **策略更新**：使用聚合数据集 $D_{agg}$ 重新训练策略，得到新的策略 $\pi_{k+1}$。

3.  **终止**：重复上述过程直到策略收敛或达到预设的迭代次数。

DAgger的关键在于，它在训练数据中包含了机器人自身策略所访问到的状态，从而确保了学习到的策略在这些状态下也能表现良好，有效缓解了协变量偏移 [5]。

### 3.2 Sequential Learning Reductions

DAgger算法可以被看作是**序列学习归约 (Sequential Learning Reductions)** 的一个实例。序列学习归约是一种理论框架，它将复杂的序列决策问题（如模仿学习）转化为一系列更简单的、独立的监督学习问题 [6]。通过这种归约，可以利用成熟的监督学习算法来解决序列决策问题，并提供理论上的性能保证。

DAgger通过将在线收集的机器人状态及其对应的专家动作添加到训练集中，实际上是将一个在线学习问题（机器人与环境交互）归约为一个离线监督学习问题（在聚合数据集上训练），并迭代地进行这个过程。

## 4. 代码示例 (DAgger概念)

以下是一个概念性的Python代码片段，用于说明DAgger算法的基本流程。我们模拟一个简单的导航任务，并展示机器人如何通过与专家的交互来改进其策略。

```python
import numpy as np
from sklearn.linear_model import LinearRegression

class Expert:
    def get_action(self, state, target_pos):
        # 专家总是向目标移动
        direction = target_pos - state
        return direction / np.linalg.norm(direction) if np.linalg.norm(direction) > 0 else np.array([0.0, 0.0])

class Policy:
    def __init__(self):
        self.model = LinearRegression()
        self.is_trained = False

    def train(self, states, actions):
        print("训练策略...")
        self.model.fit(states, actions)
        self.is_trained = True

    def predict_action(self, state):
        if not self.is_trained:
            # 初始时随机动作或默认动作
            return np.random.rand(2) * 2 - 1 # 随机动作
        return self.model.predict(state.reshape(1, -1))[0]

def dagger_algorithm(num_iterations=5, num_expert_demos=10, num_robot_steps=20):
    expert = Expert()
    policy = Policy()
    target_pos = np.array([10.0, 10.0])

    # 1. 初始化：收集初始专家演示
    print("\n--- 阶段1: 收集初始专家演示 ---")
    aggregated_states = []
    aggregated_actions = []
    for _ in range(num_expert_demos):
        current_state = np.random.rand(2) * 5 # 随机起始状态
        for _ in range(num_robot_steps):
            action = expert.get_action(current_state, target_pos)
            aggregated_states.append(current_state)
            aggregated_actions.append(action)
            current_state += action * 0.5 + np.random.randn(2) * 0.1 # 专家移动
            if np.linalg.norm(current_state - target_pos) < 1.0: break
    
    aggregated_states = np.array(aggregated_states)
    aggregated_actions = np.array(aggregated_actions)
    policy.train(aggregated_states, aggregated_actions)

    # 2. 迭代过程
    for i in range(num_iterations):
        print(f"\n--- 阶段2: DAgger迭代 {i+1}/{num_iterations} ---")
        robot_states_visited = []
        current_robot_state = np.array([0.0, 0.0]) # 机器人从固定起点开始
        print(f"机器人从状态 {current_robot_state.round(2)} 开始执行...")

        for step in range(num_robot_steps):
            # a. 策略执行
            action_by_policy = policy.predict_action(current_robot_state)
            robot_states_visited.append(current_robot_state)
            
            # b. 专家标注 (模拟：专家提供正确动作)
            expert_action_for_this_state = expert.get_action(current_robot_state, target_pos)
            aggregated_states = np.vstack([aggregated_states, current_robot_state])
            aggregated_actions = np.vstack([aggregated_actions, expert_action_for_this_state])

            current_robot_state += action_by_policy * 0.5 + np.random.randn(2) * 0.05 # 机器人移动
            print(f"  步 {step+1}: 机器人状态={current_robot_state.round(2)}, 策略动作={action_by_policy.round(2)}, 专家动作={expert_action_for_this_state.round(2)}")
            if np.linalg.norm(current_robot_state - target_pos) < 1.0: 
                print("  机器人到达目标！")
                break
        
        # c. 数据聚合和 d. 策略更新
        policy.train(aggregated_states, aggregated_actions)
        print(f"  聚合数据集大小: {len(aggregated_states)} 条")

    print("\n--- DAgger算法完成 ---")
    print("最终策略已训练，可以部署。")

if __name__ == "__main__":
    dagger_algorithm()
```

## 5. 参考资料

- [1] Celemin, C., & Kober, J. (2022). Interactive Imitation Learning in Robotics: A Survey. *Foundations and Trends® in Robotics*, 10(1-2), 1-134. [PDF](http://www.jenskober.de/publications/Celemin2022FTR.pdf)
- [2] Cai, H., et al. (2025). Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism. *International Conference on Machine Learning (ICML)*. [URL](https://openreview.net/forum?id=TC1sQg5z0T)
- [3] Welte, E., et al. (2025). Interactive imitation learning for dexterous robotic manipulation. *Frontiers in Robotics and AI*, 12, 1682437. [URL](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1682437/full)
- [4] Ross, S., Gordon, G., & Bagnell, D. (2011). A reduction of imitation learning and structured prediction to no-regret online learning. *Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS)*. [PDF](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)
- [5] Nvidia. (n.d.). *Imitation Learning*. Retrieved from [URL](https://www.nvidia.com/en-us/glossary/imitation-learning/)
- [6] Judah, K., et al. (2014). Active Imitation Learning: Formal and Practical Reductions to Active i.i.d. Learning. *Journal of Machine Learning Research*, 15, 2927-2953. [PDF](https://jmlr.org/papers/volume15/judah14a/judah14a.pdf)
