# 4.2 2D平面抓取

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

2D平面抓取是机器人抓取问题的一个简化但重要的子集，它假设机器人夹持器（通常是两指夹持器）在二维平面内对物体进行抓取。尽管是2D，但它在许多工业应用中仍然非常实用，例如平面上的物体分拣、装配等。基于深度学习的2D平面抓取方法通过直接从RGB或深度图像中预测抓取姿态（通常是抓取中心、开口宽度和旋转角度），极大地提高了抓取的鲁棒性和效率 [1]。

本节将介绍2D平面抓取的核心概念，并详细探讨几种经典的深度学习模型，如GG-CNN、GR-ConvNet，以及相关的NBMOD和Grasp-Det-Seg方法，并讨论2D平面抓取的数据集和评估指标。

## 2. 核心原理

2D平面抓取通常将抓取表示为一个**抓取矩形 (Grasp Rectangle)**，它定义了夹持器在图像平面上的位置、方向和开口宽度。一个抓取矩形通常由以下参数描述 [2]：

-   **中心点 (x, y)**：抓取在图像中的中心坐标。
-   **旋转角度 (θ)**：抓取矩形相对于水平轴的旋转角度。
-   **开口宽度 (w)**：夹持器张开的宽度。
-   **抓取深度 (d)**：夹持器相对于图像平面的深度（如果使用深度信息）。

深度学习模型的目标是接收原始图像（RGB或深度图）作为输入，并输出这些抓取参数，或者直接输出每个像素作为抓取中心的抓取质量和姿态。

```mermaid
graph TD
    A[输入图像 (RGB/深度图)] --> B{深度学习模型 (CNN)}
    B --> C[特征提取]
    C --> D[抓取参数预测 (x, y, θ, w, d)]
    D --> E[抓取质量评估]
    E --> F[最佳抓取姿态]

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#afa,stroke:#333,stroke-width:2px
    style E fill:#ffc,stroke:#333,stroke-width:2px
    style F fill:#9f9,stroke:#333,stroke-width:2px
```

## 3. 关键方法/算法

### 3.1 GG-CNN (Generative Grasping Convolutional Neural Network)

GG-CNN是一种轻量级的全卷积网络，它能够从单张深度图像中实时预测每个像素的抓取质量和姿态 [3]。其核心思想是生成式地预测抓取，即对于图像中的每个像素，网络都会预测一个可能的抓取矩形及其质量。这使得GG-CNN能够处理杂乱场景中的多种物体，并快速找到可行的抓取。

GG-CNN的输出通常包括：

-   **抓取质量图 (Grasp Quality Map)**：表示每个像素作为抓取中心的成功概率。
-   **抓取角度图 (Grasp Angle Map)**：表示每个像素作为抓取中心的最佳旋转角度。
-   **抓取宽度图 (Grasp Width Map)**：表示每个像素作为抓取中心的最佳开口宽度。

### 3.2 GR-ConvNet (Generative Residual Convolutional Neural Network)

GR-ConvNet是GG-CNN的改进版，它引入了残差连接，并能够处理多通道输入（如RGB-D图像），从而提高了抓取预测的准确性和鲁棒性 [4]。GR-ConvNet同样采用生成式方法，为每个像素生成抓取候选，并通过残差结构更好地学习图像特征。

GR-ConvNet在杂乱场景和未知物体抓取方面表现出色，其实时性能使其适用于实际机器人系统 [5]。

### 3.3 NBMOD (Neural Bounding Box for Object Detection)

NBMOD是一种用于目标检测的方法，虽然不直接是抓取算法，但其目标检测能力可以为抓取提供物体的位置信息。在2D平面抓取中，NBMOD可以用于识别图像中的物体，并提供其2D边界框，然后抓取算法可以在这些边界框内寻找最佳抓取 [6]。

### 3.4 Grasp-Det-Seg (Grasp Detection and Segmentation)

Grasp-Det-Seg是一种结合了抓取检测和语义分割的方法。它不仅预测抓取姿态，还同时对图像中的物体进行分割。通过语义分割，机器人可以更好地理解场景中的物体边界和类别，从而避免抓取到背景或不相关的物体，提高抓取成功率 [7]。

## 4. 2D平面抓取数据集与指标

### 4.1 常用数据集

-   **Cornell Grasp Dataset**：一个经典的2D平面抓取数据集，包含大量RGB-D图像和人工标注的抓取矩形。它常用于评估2D平面抓取算法的性能 [8]。
-   **Jacquard Dataset**：一个大规模的合成抓取数据集，包含超过一百万个抓取，覆盖了各种物体和抓取姿态，为深度学习模型的训练提供了丰富的资源 [9]。

### 4.2 评估指标

-   **抓取成功率 (Grasp Success Rate)**：最直观的指标，表示机器人成功抓取物体的次数占总尝试次数的比例。
-   **平均精度 (Average Precision, AP)**：类似于目标检测中的AP，用于评估抓取检测的准确性，考虑了预测抓取与真实抓取之间的重叠度。
-   **IOU (Intersection over Union)**：衡量预测抓取矩形与真实抓取矩形之间的重叠程度。

## 5. 代码示例 (GG-CNN概念)

以下是一个概念性的Python代码片段，用于说明GG-CNN的输入和输出结构。它模拟了一个简单的卷积过程，并展示了如何从输入图像中生成抓取质量图和角度图。这并非一个完整的GG-CNN实现，而是为了说明其核心机制。

```python
import torch
import torch.nn as nn
import numpy as np

class SimpleGGCNN(nn.Module):
    def __init__(self):
        super(SimpleGGCNN, self).__init__()
        # 模拟一个简单的卷积层，将输入深度图转换为特征图
        # 假设输入深度图是 1xH_in x W_in
        # 输出特征图是 64xH_out x W_out
        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        
        # 模拟生成抓取质量图和角度图的层
        # 假设输出与输入特征图尺寸相同
        self.quality_output = nn.Conv2d(64, 1, kernel_size=1)
        self.angle_output = nn.Conv2d(64, 1, kernel_size=1)
        self.width_output = nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, depth_image):
        # depth_image: (batch_size, 1, H, W)
        features = self.relu(self.conv1(depth_image))
        
        # 预测抓取质量、角度和宽度
        grasp_quality = torch.sigmoid(self.quality_output(features)) # 0-1之间
        grasp_angle = self.angle_output(features) # 角度可以是任意值，通常归一化到-pi/2到pi/2
        grasp_width = torch.relu(self.width_output(features)) # 宽度必须是非负数
        
        return grasp_quality, grasp_angle, grasp_width

if __name__ == '__main__':
    # 模拟一个输入深度图像 (Batch x Channel x Height x Width)
    # 例如，一个批次中一张 224x224 的深度图
    dummy_depth_image = torch.randn(1, 1, 224, 224)

    # 实例化模型
    model = SimpleGGCNN()

    # 前向传播
    quality_map, angle_map, width_map = model(dummy_depth_image)

    print("Input Depth Image Shape:", dummy_depth_image.shape)
    print("Grasp Quality Map Shape:", quality_map.shape)
    print("Grasp Angle Map Shape:", angle_map.shape)
    print("Grasp Width Map Shape:", width_map.shape)

    # 假设我们想找到最高质量的抓取
    max_quality, idx = torch.max(quality_map.view(-1), 0)
    # 将一维索引转换回二维 (H, W)
    h, w = quality_map.shape[2:]
    best_y = idx // w
    best_x = idx % w

    print(f"\nBest Grasp Quality: {max_quality.item():.4f} at pixel ({best_x.item()}, {best_y.item()})")
    print(f"Predicted Angle at best pixel: {angle_map[0, 0, best_y, best_x].item():.4f} radians")
    print(f"Predicted Width at best pixel: {width_map[0, 0, best_y, best_x].item():.4f} pixels")

    # 实际应用中，会根据这些图选择一个最佳抓取，并将其转换为机器人可执行的指令。
```

## 6. 参考资料

- [1] Lenz, I., Lee, H., & Saxena, A. (2015). Deep Learning for Detecting Robotic Grasps. *The International Journal of Robotics Research*, 34(4-5), 705-724.
- [2] Kumra, S., Kanan, C., & Hsiao, K. (2019). Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network. *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.
- [3] Morrison, D., et al. (2018). Closing the Loop for Robotic Grasping: A Real-time, Generative, Volumetric Grasping Network. *IEEE International Conference on Robotics and Automation (ICRA)*. [arXiv:1804.05172](https://arxiv.org/abs/1804.05172)
- [4] Kumra, S., Kanan, C., & Hsiao, K. (2019). Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network. *arXiv preprint arXiv:1909.04810*.
- [5] Loahit5101. (n.d.). *GR-ConvNet-grasping*. GitHub. Retrieved from [URL](https://github.com/Loahit5101/GR-ConvNet-grasping)
- [6] Jiang, P., et al. (2020). Depth Image–Based Deep Learning of Grasp Planning for Robotic Bin Picking. *Sensors*, 20(4), 1087. [PMC7038393](https://pmc.ncbi.nlm.nih.gov/articles/PMC7038393/)
- [7] Yin, Z., et al. (2022). Overview of robotic grasp detection from 2D to 3D. *Robotics and Autonomous Systems*, 151, 104033.
- [8] Cornell University. (n.d.). *Cornell Grasp Dataset*. Retrieved from [URL](http://pr.cs.cornell.edu/grasping/rect_data/data.php)
- [9] Mahler, J., et al. (2017). Dex-Net 2.0: Learning to Grasp Arbitrary Objects with Deep Reinforcement Learning. *Robotics: Science and Systems (RSS)*.
