# 7.3 ACT与变体，Diffusion Policy

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

在具身智能和机器人学习领域，如何高效、稳定地学习视觉运动策略（Visuomotor Policy）是核心挑战之一。传统的单步动作预测方法往往难以捕捉复杂任务中的时序依赖性和长期规划。近年来，**Action Chunking with Transformers (ACT)** 和 **Diffusion Policy** 作为两种创新的方法，为解决这些问题提供了新的思路，并在机器人抓取与操作等任务中取得了显著进展 [1]。

本节将详细介绍 ACT 和 Diffusion Policy 的核心原理、关键技术及其在机器人领域的应用，并探讨它们如何共同推动具身智能的发展。

## 2. Action Chunking with Transformers (ACT)

### 2.1 定义与核心思想

**Action Chunking with Transformers (ACT)** 是一种基于 Transformer 架构的模仿学习方法，旨在通过预测一系列连续的动作（即“动作块”或“Action Chunk”）来提高机器人策略的学习效率和泛化能力 [2]。与传统的每次只预测一个动作的策略不同，ACT 每次预测一个短序列的未来动作，从而更好地捕捉任务中的时序依赖性，并使策略更加连贯和高效。

ACT 的核心思想是：

-   **动作块预测**：将机器人控制问题转化为一个序列到序列的预测任务，其中输入是当前观测（视觉、本体感受等），输出是一个包含未来 $K$ 个动作的序列。
-   **Transformer 架构**：利用 Transformer 的自注意力机制来处理输入观测和输出动作序列，捕捉它们之间的复杂关系。Transformer 能够有效地处理长距离依赖，这对于预测动作块至关重要。

### 2.2 关键技术

1.  **Transformer Encoder-Decoder**：ACT 通常采用 Encoder-Decoder 结构的 Transformer。Encoder 接收当前状态的视觉和本体感受特征，并将其编码为上下文表示。Decoder 则利用这个上下文表示和自注意力机制，逐步生成一个动作块。
2.  **动作块 (Action Chunk)**：一个动作块包含 $K$ 个连续的动作。预测动作块而不是单个动作，可以减少策略的决策频率，从而提高计算效率，并有助于学习更平滑、更协调的运动。
3.  **模仿学习 (Imitation Learning)**：ACT 通常通过模仿学习进行训练，即从专家演示数据中学习。模型的目标是最小化预测动作块与专家动作块之间的差异。

### 2.3 优点与应用

**优点**：
-   **提高样本效率**：通过预测动作块，ACT 能够从有限的专家演示中学习到更丰富的时序信息。
-   **增强泛化能力**：Transformer 的强大表示能力和动作块的引入，使得 ACT 能够更好地泛化到未见过的新任务和新环境。
-   **适用于精细操作**：ACT 在学习精细的双臂操作等任务中表现出色，能够生成连贯且精确的动作序列 [3]。
-   **低成本机器人适用**：ACT 的轻量级和高效性使其适用于低成本机器人平台 [4]。

**应用**：
-   **双臂协作任务**：如 ALOHA (A Low-cost Open-source Hardware System for Bimanual Teleoperation) 系统中，ACT 被用于学习复杂的双臂操作策略 [5]。
-   **精细抓取与放置**：在需要高精度和协调性的抓取任务中，ACT 能够生成有效的动作序列。
-   **机器人装配**：学习多步骤的装配任务，如插入、拧螺丝等。

## 3. Diffusion Policy

### 3.1 定义与核心思想

**Diffusion Policy** 是一种将机器人策略表示为条件去噪扩散模型 (Conditional Denoising Diffusion Model) 的方法 [6]。它借鉴了生成式扩散模型在图像生成领域的成功，将其应用于机器人动作序列的生成。Diffusion Policy 的核心思想是：将机器人动作序列视为一种“数据”，并通过学习一个逆向扩散过程，从随机噪声中逐步“去噪”出符合当前观测和任务指令的动作序列。

### 3.2 关键技术

1.  **条件去噪扩散模型**：Diffusion Policy 的核心是一个条件扩散模型。在训练阶段，模型学习如何从一个带噪声的动作序列中预测出原始的动作序列，同时以当前视觉观测、本体感受信息和任务指令作为条件。
2.  **逐步生成**：在推理阶段，模型从一个随机采样的噪声动作序列开始，通过多次迭代应用学习到的去噪网络，逐步将噪声转化为一个连贯、有效的机器人动作序列。
3.  **多模态条件**：视觉观测（图像）、本体感受（关节角度、速度）和语言指令等信息被编码并作为条件输入到扩散模型中，指导动作序列的生成。

### 3.3 优点与应用

**优点**：
-   **生成多样化行为**：扩散模型天生具有生成多样化样本的能力，使得 Diffusion Policy 能够生成多种可能的有效动作序列，这对于处理不确定性或需要灵活应对的任务非常有用。
-   **处理多模态输入**：能够有效地整合视觉、本体感受和语言等多种模态信息来指导动作生成。
-   **鲁棒性**：通过去噪过程，Diffusion Policy 对输入噪声和环境扰动具有一定的鲁棒性。
-   **高样本效率**：在某些任务中，Diffusion Policy 能够以相对较少的演示数据学习到高质量的策略 [7]。

**应用**：
-   **视觉运动控制**：直接从图像输入生成机器人控制指令，实现端到端的视觉运动控制。
-   **复杂操作任务**：如抓取、放置、推拉等，尤其是在需要精细控制和适应性的场景。
-   **多任务学习**：通过条件输入，一个 Diffusion Policy 可以学习执行多种不同的任务。

## 4. ACT与Diffusion Policy的比较与结合

| 特性       | Action Chunking with Transformers (ACT)                | Diffusion Policy                                         |
| :--------- | :----------------------------------------------------- | :------------------------------------------------------- |
| **核心机制** | Transformer 预测动作块。                               | 条件去噪扩散模型生成动作序列。                         |
| **学习范式** | 模仿学习。                                             | 模仿学习（通过学习数据分布）。                         |
| **动作生成** | 直接预测离散或连续的动作序列。                         | 从噪声中逐步去噪生成动作序列。                         |
| **多样性**   | 相对较低，通常生成确定性或近似确定性动作。             | 较高，天生具有生成多样化动作的能力。                   |
| **鲁棒性**   | 依赖于 Transformer 的鲁棒性。                          | 通过去噪过程对噪声和扰动具有较强鲁棒性。             |
| **计算**   | 每次推理生成一个动作块。                               | 每次推理需要多次迭代去噪。                             |

**结合点**：

ACT 和 Diffusion Policy 并非相互排斥，它们可以相互借鉴或结合使用。例如，Transformer 架构可以作为 Diffusion Policy 中的去噪网络，利用其强大的序列建模能力来提高动作生成的质量。一些研究也探索了将动作块的概念融入到扩散模型中，以进一步提高效率和连贯性。

## 5. 机器人应用

ACT 和 Diffusion Policy 在机器人抓取与操作中都取得了令人印象深刻的成果：

-   **高精度抓取**：无论是 ACT 预测的连贯动作块，还是 Diffusion Policy 生成的精细动作序列，都能够帮助机器人实现对复杂形状和易碎物体的高精度抓取。
-   **灵巧操作**：在需要多关节协调和精细触觉反馈的任务中，这两种方法能够学习到人类专家级别的操作技能。
-   **多任务泛化**：通过结合视觉和语言指令，这些策略能够泛化到未见过的新物体和新任务，例如根据语言指令抓取特定颜色的物体或执行特定顺序的操作。
-   **Sim-to-Real 迁移**：由于它们能够从大规模数据中学习，并且对噪声具有一定的鲁棒性，因此在仿真环境中训练的 ACT 或 Diffusion Policy 策略可以更好地迁移到真实机器人上。

## 6. 代码示例 (概念性 Diffusion Policy 训练)

以下是一个概念性的 Python 代码片段，展示了 Diffusion Policy 的训练过程。这里我们简化了环境和模型，以突出核心思想：如何训练一个网络来从噪声中恢复原始数据（动作序列）。

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 模拟一个简单的动作序列 (例如，一个二维轨迹)
def generate_expert_trajectory(length=10):
    trajectory = []
    x, y = 0.0, 0.0
    for _ in range(length):
        x += np.random.rand() * 0.1 - 0.05 # 小幅随机移动
        y += np.random.rand() * 0.1 - 0.05
        trajectory.append([x, y])
    return torch.tensor(trajectory, dtype=torch.float32)

# 模拟扩散过程中的噪声调度
def linear_beta_schedule(timesteps):
    scale = 1000 / timesteps
    beta_start = scale * 0.0001
    beta_end = scale * 0.02
    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)

def get_alphas_betas(timesteps):
    betas = linear_beta_schedule(timesteps)
    alphas = 1. - betas
    alphas_cumprod = torch.cumprod(alphas, axis=0)
    return alphas, alphas_cumprod

# 模拟去噪网络 (例如，一个简单的MLP)
class DenoiseNet(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )

    def forward(self, x, t): # x是带噪声的动作序列，t是时间步
        # 在实际的Diffusion Policy中，t会作为条件嵌入到网络中
        # 这里简化处理，只用x
        return self.net(x)

# 训练 Diffusion Policy
def train_diffusion_policy(denoise_net, num_epochs=1000, timesteps=100, learning_rate=1e-3):
    optimizer = optim.Adam(denoise_net.parameters(), lr=learning_rate)
    loss_fn = nn.MSELoss()
    
    alphas, alphas_cumprod = get_alphas_betas(timesteps)

    print("--- Diffusion Policy 训练开始 (概念性) ---")
    for epoch in range(num_epochs):
        # 1. 生成一个专家动作序列 (x0)
        x0 = generate_expert_trajectory(length=10).flatten() # 展平为一维向量
        
        # 2. 随机选择一个时间步 t
        t = torch.randint(0, timesteps, (1,)).long()

        # 3. 计算在时间步 t 对应的噪声 (epsilon)
        # x_t = sqrt(alpha_cumprod_t) * x0 + sqrt(1 - alpha_cumprod_t) * epsilon
        # epsilon = (x_t - sqrt(alpha_cumprod_t) * x0) / sqrt(1 - alpha_cumprod_t)
        # 为了训练，我们从x0和随机噪声epsilon中生成x_t，然后让网络预测epsilon
        noise = torch.randn_like(x0)
        sqrt_alpha_cumprod_t = alphas_cumprod[t].sqrt()
        sqrt_one_minus_alpha_cumprod_t = (1. - alphas_cumprod[t]).sqrt()
        x_t = sqrt_alpha_cumprod_t * x0 + sqrt_one_minus_alpha_cumprod_t * noise

        # 4. 去噪网络预测噪声
        predicted_noise = denoise_net(x_t, t)

        # 5. 计算损失 (预测噪声与真实噪声的MSE)
        loss = loss_fn(predicted_noise, noise)

        # 6. 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 100 == 0:
            print(f"Epoch {epoch+1}: Loss = {loss.item():.6f}")
    print("--- Diffusion Policy 训练结束 ---")

# 概念性 Diffusion Policy 推理 (生成动作序列)
def generate_action_sequence(denoise_net, trajectory_dim=20, timesteps=100):
    alphas, alphas_cumprod = get_alphas_betas(timesteps)
    sqrt_recip_alphas = (1.0 / alphas).sqrt()
    posterior_variance = betas * (1. - alphas_cumprod.roll(1)) / (1. - alphas_cumprod)
    posterior_variance[0] = betas[0]

    # 从纯噪声开始
    x = torch.randn(trajectory_dim)

    print("--- Diffusion Policy 生成动作序列 (概念性) ---")
    for i in range(timesteps - 1, -1, -1):
        t = torch.tensor([i], dtype=torch.long)
        # 预测噪声
        predicted_noise = denoise_net(x, t)

        # 从x_t恢复x_{t-1} (DDPM的逆向过程)
        mean = sqrt_recip_alphas[t] * (x - betas[t] / (1. - alphas_cumprod[t]).sqrt() * predicted_noise)
        if i > 0:
            noise = torch.randn_like(x)
            x = mean + (posterior_variance[t].sqrt()) * noise
        else:
            x = mean
        
        if (timesteps - i) % 20 == 0 or i == 0:
            print(f"  Step {timesteps - i}: Current action chunk (first 2 elements): {x[:2].tolist()}")

    return x.reshape(-1, 2) # 恢复为二维轨迹

if __name__ == "__main__":
    trajectory_length = 10
    action_dim = 2 # x, y 坐标
    input_dim = trajectory_length * action_dim # 展平后的动作序列维度
    hidden_dim = 128
    timesteps = 100

    denoise_net = DenoiseNet(input_dim, hidden_dim)
    train_diffusion_policy(denoise_net, num_epochs=2000, timesteps=timesteps)

    generated_trajectory = generate_action_sequence(denoise_net, trajectory_dim=input_dim, timesteps=timesteps)
    print("\n生成的动作序列 (前5步):")
    print(generated_trajectory[:5].tolist())
```

## 7. 参考资料

- [1] Chi, C., et al. (2023). Visuomotor Policy Learning via Action Diffusion. *Robotics: Science and Systems (RSS)*.
- [2] Mandlekar, A., et al. (2022). Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. *arXiv preprint arXiv:2304.13705*.
- [3] TonyZhaozh. (n.d.). *ALOHA: A Low-cost Open-source Hardware System for Bimanual Teleoperation*. [URL](https://tonyzhaozh.github.io/aloha/)
- [4] Shaka-Labs. (n.d.). *Action Chunking Transformer implementation for low cost robot*. GitHub. [URL](https://github.com/Shaka-Labs/ACT)
- [5] Mandlekar, A., et al. (2022). Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. *arXiv preprint arXiv:2304.13705*.
- [6] Chi, C., et al. (2023). Visuomotor Policy Learning via Action Diffusion. *Robotics: Science and Systems (RSS)*.
- [7] Diffusion Policy. (n.d.). *Diffusion Policy*. [URL](https://diffusion-policy.cs.columbia.edu/)
