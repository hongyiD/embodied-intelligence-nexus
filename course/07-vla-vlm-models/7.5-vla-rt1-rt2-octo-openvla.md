# 7.5 VLA: RT1, RT2, Octo和OpenVLA

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

在具身智能领域，构建能够处理视觉、语言和动作的通用机器人模型是当前研究的核心目标。为了实现这一目标，研究者们提出了多种具有里程碑意义的视觉-语言-动作 (Vision-Language-Action, VLA) 模型。本节将重点介绍 Google DeepMind 的 **RT-1 (Robotics Transformer 1)** 和 **RT-2 (Robotics Transformer 2)**，以及由多个研究机构合作推出的 **Octo** 模型，并探讨它们如何推动通用机器人策略的发展。

## 2. RT-1: Robotics Transformer 1

### 2.1 定义与核心思想

**RT-1 (Robotics Transformer 1)** 是 Google Research 于2022年推出的一种机器人学习方法，旨在实现大规模、真实世界的机器人控制 [1]。RT-1 的核心思想是将机器人控制任务视为一个序列预测问题，并利用 **Transformer 架构**来处理多模态输入（图像、文本指令）和生成离散的动作序列。

RT-1 的关键创新在于：

-   **统一的 tokenization**：将所有输入（包括视觉观测、语言指令）和输出（机器人动作）都转化为离散的 token 序列，使得 Transformer 能够以统一的方式处理不同模态的数据。
-   **大规模数据集训练**：RT-1 在包含数万个真实世界机器人演示的大规模数据集上进行训练，从而学习到强大的泛化能力。
-   **端到端控制**：直接从原始传感器数据（如摄像头图像）和语言指令生成机器人控制信号，实现了端到端的视觉运动控制。

### 2.2 特点

-   **可扩展性**：Transformer 架构的固有可扩展性使得 RT-1 能够从更多数据中受益，并处理更复杂的任务。
-   **泛化能力**：通过在多样化数据集上训练，RT-1 展现了对新物体、新环境和新任务的良好泛化能力 [2]。
-   **真实世界部署**：RT-1 被设计用于真实世界的机器人，能够处理各种实际操作任务，如抓取、放置、开抽屉等。

### 2.3 应用

RT-1 主要应用于多任务机器人控制，尤其是在家庭和实验室环境中执行各种日常操作。它能够理解自然语言指令，并将其转化为具体的机器人动作，例如“把薯片放到抽屉里”或“把水瓶从桌子上拿下来”。

## 3. RT-2: Vision-Language-Action Models

### 3.1 定义与核心思想

**RT-2 (Robotics Transformer 2)** 是 Google DeepMind 在 RT-1 的基础上进一步发展而来的 **视觉-语言-动作 (VLA) 模型** [3]。RT-2 的核心思想是利用在互联网规模数据上预训练的 **视觉-语言模型 (VLM)** 的强大知识，并将其直接融入到端到端的机器人控制策略中，从而显著提升机器人的泛化能力和推理能力。

RT-2 的关键在于：

-   **VLM 知识迁移**：RT-2 将在海量网络图像和文本数据上训练的 VLM（如 PaLM-E）作为其骨干网络，从而继承了 VLM 对世界知识的理解能力。
-   **直接生成动作**：RT-2 能够直接将视觉观测和语言指令转化为机器人动作，而无需显式的中间表示或规划步骤。
-   **指令遵循与推理**：通过 VLM 的能力，RT-2 能够更好地理解抽象的语言指令，并进行一定程度的常识推理，例如理解“把垃圾扔掉”意味着将物体放入垃圾桶。

### 3.2 特点

-   **从 VLM 到 VLA 的桥梁**：RT-2 有效地将视觉-语言理解能力扩展到动作生成，是 VLM 赋能具身智能的关键一步。
-   **提升泛化能力**：通过利用互联网规模的知识，RT-2 在处理未见过的新物体、新场景和新任务时表现出更强的泛化能力 [4]。
-   **多模态输入**：能够同时处理图像和文本指令，实现更灵活的机器人交互。

### 3.3 应用

RT-2 在各种机器人任务中展现了强大的能力，包括：

-   **复杂指令遵循**：执行包含抽象概念的指令，如“把脏东西扔掉”。
-   **新物体操作**：无需额外训练即可操作从未见过的物体。
-   **环境适应**：在不同光照、背景和干扰的环境中稳定执行任务。

## 4. Octo: An Open-Source Generalist Robot Policy

### 4.1 定义与核心思想

**Octo** 是一个由多个研究机构合作开发的**开源通用机器人策略 (Open-Source Generalist Robot Policy)** [5]。它是一个基于 **Transformer 的扩散策略 (Transformer-based Diffusion Policy)**，在迄今为止最大的机器人操作数据集 **Open X-Embodiment** 上进行了预训练。Octo 的目标是提供一个统一的、可泛化的机器人控制模型，能够适用于多种机器人平台和任务。

Octo 的核心特点包括：

-   **大规模预训练**：在包含80万条机器人轨迹的 Open X-Embodiment 数据集上进行训练，涵盖了多种机器人类型、任务和环境。
-   **Transformer-based Diffusion Policy**：结合了 Transformer 的序列建模能力和扩散模型的生成多样性，能够生成高质量、鲁棒的动作序列。
-   **通用性**：旨在成为一个“基础模型”，通过微调即可适应新的机器人平台和特定任务，而无需从头开始训练。

### 4.2 特点

-   **开源**：模型的代码、权重和训练数据都已开源，促进了社区的研究和应用。
-   **多机器人兼容**：通过统一的动作表示和条件化机制，Octo 能够控制不同构型和传感器的机器人。
-   **高样本效率**：预训练模型能够显著减少在新任务上进行微调所需的样本量。

### 4.3 应用

Octo 的应用范围非常广泛，包括：

-   **多任务机器人操作**：从简单的抓取到复杂的组装任务。
-   **新环境适应**：通过少量演示即可在新环境中执行任务。
-   **作为基础模型**：为研究者提供一个强大的起点，用于开发更专业的机器人策略。

## 5. OpenVLA

**OpenVLA** 是一个与 Octo 紧密相关的概念，它代表了构建**开放、通用、可扩展的视觉-语言-动作模型**的愿景。虽然没有一个独立的“OpenVLA”模型像 RT-1/RT-2 或 Octo 那样被明确命名和发布，但 Octo 本身就是这一愿景的体现，它提供了一个开源的通用 VLA 策略，旨在成为未来开放 VLA 生态系统的基石 [6]。

OpenVLA 的理念强调：

-   **开放性**：鼓励社区贡献数据、模型和工具，共同推动 VLA 技术的发展。
-   **通用性**：开发能够跨越不同机器人硬件、任务和环境的统一模型。
-   **可扩展性**：模型应能够从更多数据和计算资源中持续学习和改进。

## 6. 模型比较

| 特性         | RT-1                                       | RT-2                                               | Octo                                                 |
| :----------- | :----------------------------------------- | :------------------------------------------------- | :--------------------------------------------------- |
| **发布时间** | 2022年                                     | 2023年                                             | 2024年                                               |
| **核心思想** | Transformer处理多模态输入，生成离散动作。 | 结合互联网VLM知识，直接生成动作。                  | Transformer-based Diffusion Policy，大规模预训练。   |
| **数据来源** | 大规模真实世界机器人演示数据。             | 互联网VLM数据 + 机器人数据。                       | Open X-Embodiment (80万轨迹)。                       |
| **架构**     | Transformer。                              | VLM (如 PaLM-E) 作为骨干 + Transformer。           | Transformer + Diffusion Model。                      |
| **主要优势** | 良好的可扩展性和真实世界泛化能力。         | 强大的泛化和推理能力，利用互联网知识。             | 通用性、开源、多机器人兼容、高样本效率。             |
| **目标**     | 大规模真实世界机器人控制。                 | 将VLM能力扩展到动作生成，提升泛化。                | 提供统一、可泛化的开源机器人策略。                   |

## 7. 代码示例 (概念性 VLA 模型推理)

以下是一个概念性的 Python 代码示例，展示了如何使用一个假想的 VLA 模型进行推理。这个示例模拟了模型接收视觉观测和语言指令，然后生成机器人动作的过程。实际的 VLA 模型通常会涉及复杂的模型加载、特征提取和动作解码。

```python
import torch
import numpy as np

# 假设这是一个预训练的VLA模型
class ConceptualVLAModel(torch.nn.Module):
    def __init__(self, visual_feature_dim=512, lang_feature_dim=512, action_dim=7):
        super().__init__()
        # 模拟视觉编码器
        self.visual_encoder = torch.nn.Linear(1024, visual_feature_dim)
        # 模拟语言编码器
        self.language_encoder = torch.nn.Linear(768, lang_feature_dim)
        # 模拟多模态融合和策略网络
        self.policy_head = torch.nn.Sequential(
            torch.nn.Linear(visual_feature_dim + lang_feature_dim, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, action_dim) # 例如，7DoF机械臂的关节速度
        )

    def forward(self, visual_obs, language_instruction):
        # 1. 编码视觉观测
        visual_features = self.visual_encoder(visual_obs)
        
        # 2. 编码语言指令
        language_features = self.language_encoder(language_instruction)
        
        # 3. 融合多模态特征
        fused_features = torch.cat([visual_features, language_features], dim=-1)
        
        # 4. 生成动作
        actions = self.policy_head(fused_features)
        return actions

if __name__ == "__main__":
    # 实例化概念性VLA模型
    vla_model = ConceptualVLAModel()
    
    # 模拟视觉观测 (例如，来自ResNet的特征向量)
    dummy_visual_obs = torch.randn(1, 1024) 
    
    # 模拟语言指令 (例如，来自BERT的嵌入向量)
    dummy_language_instruction = torch.randn(1, 768)
    
    print("--- 概念性VLA模型推理 ---")
    # 进行推理，生成机器人动作
    predicted_actions = vla_model(dummy_visual_obs, dummy_language_instruction)
    
    print(f"视觉观测维度: {dummy_visual_obs.shape}")
    print(f"语言指令维度: {dummy_language_instruction.shape}")
    print(f"生成的机器人动作 (例如，关节速度): {predicted_actions.shape}")
    print(f"动作值: {predicted_actions.detach().numpy()}")

    # 模拟不同的指令
    print("\n--- 模拟不同指令下的动作 ---")
    dummy_language_instruction_2 = torch.randn(1, 768) # 模拟不同的语言指令
    predicted_actions_2 = vla_model(dummy_visual_obs, dummy_language_instruction_2)
    print(f"不同指令下的动作值: {predicted_actions_2.detach().numpy()}")
```

## 8. 参考资料

- [1] Brohan, A., et al. (2022). RT-1: Robotics Transformer for Real-World Control at Scale. *arXiv preprint arXiv:2212.06817*.
- [2] Google Research. (2022). *Robotics Transformer: RT-1*. [URL](https://robotics-transformer1.github.io/)
- [3] Brohan, A., et al. (2023). RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. *arXiv preprint arXiv:2307.15818*.
- [4] Google DeepMind. (2023). *RT-2: New model translates vision and language into action*. [URL](https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/)
- [5] Octo Models. (2024). *Octo: An Open-Source Generalist Robot Policy*. [URL](https://octo-models.github.io/)
- [6] Octo Models. (n.d.). *octo-models/octo*. GitHub. [URL](https://github.com/octo-models/octo)
