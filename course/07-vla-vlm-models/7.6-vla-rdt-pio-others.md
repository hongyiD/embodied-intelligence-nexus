# 7.6 VLA: RDT, Pio和其他

- **作者**: Damon Li
- **日期**: 2026年2月4日

## 1. 概述

在具身智能和机器人学习领域，除了 Google DeepMind 的 RT 系列和 Octo 等模型外，还有许多其他重要的视觉-语言-动作 (VLA) 模型正在推动通用机器人策略的发展。本节将重点介绍 **Robotics Diffusion Transformer (RDT)** 系列模型和 **Physical Intelligence (Pio)** 团队开发的模型，探讨它们在统一动作空间、大规模数据训练和跨机器人泛化方面的创新。

## 2. Robotics Diffusion Transformer (RDT)

### 2.1 RDT-1B: 扩散基础模型

**RDT-1B (Robotics Diffusion Transformer with 1.2B parameters)** 是由清华大学等机构提出的一个具有12亿参数的扩散基础模型，专注于双臂操作任务 [1]。RDT-1B 的核心创新在于将扩散模型与 Transformer 架构相结合，以处理大规模机器人演示数据，并生成高质量的机器人动作序列。

**核心思想**：
-   **扩散模型**：RDT-1B 利用扩散模型的生成能力，从噪声中逐步去噪出连贯、精细的机器人动作序列。这使得模型能够处理连续动作空间，并生成多样化的动作。
-   **Transformer 架构**：Transformer 负责处理多模态输入（如 RGB 图像和语言指令），并捕捉动作序列中的时序依赖关系。
-   **大规模预训练**：RDT-1B 在超过100万条多机器人演示轨迹上进行预训练，使其能够学习到通用的机器人操作技能 [2]。

**特点**：
-   **双臂操作**：特别针对双臂机器人操作任务进行优化，能够处理复杂的协同动作。
-   **统一动作空间**：引入了**物理可解释的统一动作空间 (Physically Interpretable Unified Action Space)**，能够统一不同机器人的动作表示，从而促进跨机器人平台的泛化 [3]。
-   **多模态条件**：能够根据语言指令和多视角图像生成动作。

### 2.2 RDT2: 零样本跨具身泛化

**RDT2** 是 RDT-1B 的后续版本，旨在实现**零样本跨具身泛化 (Zero-Shot Cross-Embodiment Generalization)** [4]。RDT2 的目标是让模型能够在未见过的机器人平台上执行简单任务，而无需额外的微调。

**核心思想**：
-   **自回归 VLA 模型**：RDT2-VQ 是一个自回归的视觉-语言-动作 (VLA) 模型，它基于 Qwen2.5-VL-7B-Instruct 进行了适配，并利用 Residual VQ (Vector Quantization) 作为动作 tokenizer [5]。
-   **大规模 UMI 双臂数据**：在大规模的 UMI (Universal Manipulation Interface) 双臂操作数据集上进行训练，进一步增强了模型的泛化能力。

**特点**：
-   **零样本泛化**：RDT2 能够将从一个机器人上学到的技能直接迁移到另一个具有不同形态的机器人上，而无需任何额外训练。
-   **统一动作表示**：通过其独特的动作 tokenizer，RDT2 能够有效地处理不同机器人的动作空间。

## 3. Pio (Physical Intelligence) 模型

**Physical Intelligence (Pio)** 团队致力于开发通用机器人基础模型，其目标是让机器人能够在混乱的真实世界环境中进行泛化操作 [6]。他们提出了 **π0 (Pi-Zero)** 和 **π0.5 (Pi-Zero-Point-Five)** 等 VLA 模型。

### 3.1 π0 (Pi-Zero): 首个通用策略

**π0** 被认为是 Pio 团队的首个通用策略模型，旨在实现通用机器人控制 [7]。

**核心思想**：
-   **大规模机器人数据**：π0 在超过1万小时的机器人数据上进行预训练，这些数据涵盖了广泛的任务和环境。
-   **VLA 架构**：模型设计为 VLA 架构，能够整合视觉、语言和动作信息。

**特点**：
-   **通用性**：旨在成为一个可以用于多种机器人和任务的基础模型。
-   **预训练检查点**：提供了多个基础 VLA 模型检查点，可用于微调以适应特定任务 [8]。

### 3.2 π0.5 (Pi-Zero-Point-Five): 开放世界泛化

**π0.5** 是在 π0 基础上发展而来的模型，专注于实现**开放世界泛化 (Open-World Generalization)** [9]。

**核心思想**：
-   **异构任务协同训练**：π0.5 通过在异构任务上进行协同训练，使其能够更好地泛化到未见过的环境和任务。
-   **VLA 架构**：同样采用 VLA 架构，强调视觉、语言和动作的紧密结合。

**特点**：
-   **强大的泛化能力**：在面对混乱、非结构化的真实世界环境时，π0.5 展现出更强的适应性和鲁棒性。

## 4. 其他VLA模型

除了上述模型，具身智能领域还有许多其他 VLA 模型和研究方向，例如：

-   **具身世界模型 (Embodied World Models)**：旨在学习环境的动力学模型，并利用该模型进行规划和策略学习。这些模型通常结合了生成模型和强化学习，能够从互联网视频等大规模数据中学习可控的动力学 [10]。
-   **多模态基础模型**：将视觉、语言、触觉等多种模态信息整合到一个统一的基础模型中，以实现更全面的环境理解和更灵活的机器人控制。

## 5. 代码示例 (概念性 RDT 动作预测)

以下是一个概念性的 Python 代码示例，模拟了 RDT 模型如何接收视觉和语言输入，并预测机器人动作序列。这个示例简化了 RDT 的复杂内部机制，但展示了其核心功能。

```python
import torch
import torch.nn as nn
import numpy as np

# 模拟一个简化的RDT模型
class ConceptualRDT(nn.Module):
    def __init__(self, visual_dim=512, lang_dim=512, action_dim=7, seq_len=16):
        super().__init__()
        self.visual_encoder = nn.Linear(1024, visual_dim) # 模拟视觉特征提取
        self.lang_encoder = nn.Linear(768, lang_dim)     # 模拟语言特征提取
        
        # 模拟Transformer Encoder，用于融合视觉和语言特征
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=visual_dim + lang_dim, nhead=8)
        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=2)
        
        # 模拟扩散模型中的去噪网络 (这里简化为一个MLP)
        self.denoising_head = nn.Sequential(
            nn.Linear(visual_dim + lang_dim + action_dim * seq_len, 512), # 输入包含条件和带噪声的动作序列
            nn.ReLU(),
            nn.Linear(512, action_dim * seq_len) # 输出预测的噪声或动作序列
        )
        self.action_dim = action_dim
        self.seq_len = seq_len

    def forward(self, visual_obs, lang_instruction, noisy_action_sequence=None, timestep=None):
        # 1. 编码视觉和语言
        visual_features = self.visual_encoder(visual_obs)
        lang_features = self.lang_encoder(lang_instruction)
        
        # 2. 融合特征 (这里简单拼接，实际可能更复杂)
        fused_features = torch.cat([visual_features, lang_features], dim=-1)
        
        # 3. Transformer处理 (这里假设fused_features是序列的一部分)
        # 实际RDT会处理更长的序列，包含历史动作和观测
        transformer_output = self.transformer_encoder(fused_features.unsqueeze(0)).squeeze(0) # 模拟序列处理
        
        # 4. 扩散去噪 (如果提供了带噪声的动作序列)
        if noisy_action_sequence is not None and timestep is not None:
            # 将条件和带噪声的动作序列拼接作为去噪网络的输入
            # 实际timestep也会被编码并作为条件
            denoising_input = torch.cat([transformer_output, noisy_action_sequence.flatten()], dim=-1)
            predicted_noise = self.denoising_head(denoising_input)
            return predicted_noise.reshape(-1, self.action_dim) # 返回预测的噪声，用于逆向扩散
        else:
            # 如果没有提供噪声序列，则模拟直接生成一个动作序列 (例如，用于初始化)
            # 实际RDT的推理是一个迭代去噪过程
            initial_action_guess = torch.randn(self.seq_len * self.action_dim)
            denoising_input = torch.cat([transformer_output, initial_action_guess], dim=-1)
            predicted_actions = self.denoising_head(denoising_input) # 这里简化为直接预测动作
            return predicted_actions.reshape(-1, self.action_dim)

if __name__ == "__main__":
    # 实例化概念性RDT模型
    rdt_model = ConceptualRDT()
    
    # 模拟视觉观测 (例如，来自ResNet的特征向量)
    dummy_visual_obs = torch.randn(1, 1024) 
    
    # 模拟语言指令 (例如，来自BERT的嵌入向量)
    dummy_lang_instruction = torch.randn(1, 768)
    
    print("--- 概念性RDT模型推理 (直接生成动作序列) ---")
    # 进行推理，生成机器人动作序列
    predicted_action_sequence = rdt_model(dummy_visual_obs, dummy_lang_instruction)
    
    print(f"视觉观测维度: {dummy_visual_obs.shape}")
    print(f"语言指令维度: {dummy_lang_instruction.shape}")
    print(f"生成的机器人动作序列 (形状: 序列长度 x 动作维度): {predicted_action_sequence.shape}")
    print(f"动作序列前5步:\n{predicted_action_sequence[:5].detach().numpy()}")

    print("\n--- 概念性RDT模型推理 (去噪过程中的一步) ---")
    # 模拟去噪过程中的一步：给定带噪声的动作序列和时间步，预测噪声
    dummy_noisy_action_sequence = torch.randn(rdt_model.seq_len, rdt_model.action_dim)
    dummy_timestep = torch.tensor([50]) # 模拟时间步
    predicted_noise = rdt_model(dummy_visual_obs, dummy_lang_instruction, dummy_noisy_action_sequence, dummy_timestep)
    print(f"预测的噪声序列形状: {predicted_noise.shape}")
```

## 6. 参考资料

- [1] Liu, S., et al. (2024). RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation. *arXiv preprint arXiv:2410.07864*.
- [2] THU-ML. (n.d.). *RoboticsDiffusionTransformer*. GitHub. [URL](https://github.com/thu-ml/RoboticsDiffusionTransformer)
- [3] Liu, S., et al. (2025). RDT-1B: A Diffusion Foundation Model for Bimanual Manipulation. *International Conference on Learning Representations (ICLR)*.
- [4] RDT Robotics. (n.d.). *RDT2: Enabling Zero-Shot Cross-Embodiment Generalization*. [URL](https://rdt-robotics.github.io/rdt2/)
- [5] Hugging Face. (n.d.). *robotics-diffusion-transformer/RDT2-VQ*. [URL](https://huggingface.co/robotics-diffusion-transformer/RDT2-VQ)
- [6] Physical Intelligence. (n.d.). *A VLA with Open-World Generalization*. [URL](https://www.pi.website/blog/pi05)
- [7] Physical Intelligence. (n.d.). *π0 : Our First Generalist Policy*. [URL](https://www.pi.website/blog/pi0)
- [8] Physical Intelligence. (n.d.). *Physical-Intelligence/openpi*. GitHub. [URL](https://github.com/Physical-Intelligence/openpi)
- [9] Physical Intelligence. (n.d.). *π0.5: a Vision-Language-Action Model for Open-World Generalization*. [URL](https://www.pi.website/blog/pi05)
- [10] Horizon Robotics. (n.d.). *Towards a Generative 3D World Engine for Embodied Intelligence*. [URL](https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html)
